{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95592bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "1000,50   df.iloc[300:501,10:26]  #['rows',column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae07a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deb4f729",
   "metadata": {},
   "source": [
    "### Q) What is Decision Tree?\n",
    "\n",
    "\n",
    "* A decision tree is a supervised machine learning algorithm used for regression and classification tasks.\n",
    "\n",
    "\n",
    "* Decision Tree works by creating a tree-like structure of if-else conditions and produce a output when it reach to the leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb84b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38519c44",
   "metadata": {},
   "source": [
    "### Q) How does Decision Tree works?\n",
    "\n",
    "\n",
    "* Decision Tree first selects a node and a condition, then split it into binary branches, observation which satisfy with the condition goes to one side and the observations which do not satisfy with the condition goes to another side, if any branch produces a leaf node then decision stops for that branch otherwise decision tree ireratively repeated this process untill all branches produces leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e23214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c2c262",
   "metadata": {},
   "source": [
    "### Q) What are decision node / internal node?\n",
    "\n",
    "\n",
    "* After the root node every node which splits the condition is called decision node or internal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de85dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "197e9ab4",
   "metadata": {},
   "source": [
    "### Q) What is if-else condition in Decision Tree?\n",
    "\n",
    "\n",
    "* Decision tree splits the nodes into binary branches, observations which satisfy the condition goes one side of the branch, the observations which do not satisfy the condition will goes another side of the branch, it is called if-else condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b211c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aefa8d05",
   "metadata": {},
   "source": [
    "### Q) What are advantages and disadvantages of decision trees?\n",
    "\n",
    "\n",
    "Advantages:-\n",
    "\n",
    "\n",
    "* Decision tree are easy to understand and explain and it can used for both classification and regression problems.\n",
    "\n",
    "\n",
    "* Decision trees can capture non-linear relationships between independent variables and dependent variable.\n",
    "\n",
    "\n",
    "* Decision trees can provide feature importance ranking on the basis of information gain, which can be useful for feature selection.\n",
    "\n",
    "Disadvantages:-\n",
    "\n",
    "\n",
    "* Decision trees can easily overfit the training data.\n",
    "\n",
    "\n",
    "* If we don't prone the decision tree it will grown more, which is difficult to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541c1f2",
   "metadata": {},
   "source": [
    "### Q) What is overfitting in decision trees, and how can you prevent it?\n",
    "\n",
    "\n",
    "* Overfitting is a condition when model captures the complex patterns and relationship of trainning data too well, which results in model perform too well on trainning data but unable to perform well on test data.\n",
    "\n",
    "\n",
    "* In decision tree overfitting occurs when decision tree grow more. We can reduce overfitting by prunning the decision tree. We can use pre-prunning and post-pruning techniques or we can select features by feature importance and put only that features into the model which contributes the most to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f8dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "028e3ffd",
   "metadata": {},
   "source": [
    "### Q) What is pruning, and why is it important in decision trees?\n",
    "\n",
    "\n",
    "* Pruning is a technique used in decision trees to reduce and prevent model from overfitting by removing the nodes and branches of decision tree that do not contribute to the model's performance. We use 2 types of prunning technique, pre-prunning and post prunning. With the help of pre-prunning technique we prune the decision tree before it is build, and with the help of post prunning techniques we prune the decision tree after it is build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf4015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae861b32",
   "metadata": {},
   "source": [
    "### Q) What is pre-prunning and post-prunning?\n",
    "\n",
    "Pre-pruning and post-pruning are two techniques used in machine learning to prevent decision trees from overfitting on training data.\n",
    "\n",
    "\n",
    "* Pre-prunning can be de done before building a decision tree by setting the parameters with their limit, like max_depth, min_samples_split, min_samples_leaf.\n",
    "\n",
    "\n",
    "* Post-prunning can be done after building a decision tree. There are several post-prunning methods available like Grid Search CV, Cost Complexity Pruning (CCP) Alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc2859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70c6dcaf",
   "metadata": {},
   "source": [
    "### Q) What is Cost Complexity Pruning Alpha?\n",
    "\n",
    "* CCP Alpha is a regularization technique, which is used to prevent overfitting. A higher value of CCP Alpha will result in a more complex tree, while a lower value will result in a simpler tree. The optimal value of CCP Alpha is usually found through cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c0ae1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c942214b",
   "metadata": {},
   "source": [
    "### Q) Can you explain the difference between classification and regression trees?\n",
    "\n",
    "\n",
    "* Classification trees is used to predict the categorical output such as class labels and regression tree is used to predict the continuous numerial output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d5f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "313f50b3",
   "metadata": {},
   "source": [
    "Q13) How do you interpret the results of a decision tree? / How can you explain the result of decision tree?\n",
    "\n",
    "\n",
    "* To interpret or explain the results of a decision tree, we can visulaize and examine the decision tree structure from the root node to the leaf nodes.\n",
    "\n",
    "\n",
    "* First we exaine the root node which is the best and important feature of the data. This feature has impact the most on the output that's why decision tree select it as root node.\n",
    "\n",
    "\n",
    "* Secondally we examine the internal nodes or decision nodes on which decision tree further split the data. Decision tree try to make every decision node as homogenous means every feature in the data can impact the output.\n",
    "\n",
    "\n",
    "* Thirdly we examine that on what conditions it will split the root node and decision nodes. These conditions are used to find the output.\n",
    "\n",
    "\n",
    "* Lastly we examine the leaf nodes which is the output given by decision tree based on the input variables. Decision tree will generate leaf nodes for all branches of decision nodes. It means it generate output for all conditions of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec5a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cc936a4",
   "metadata": {},
   "source": [
    "### Q) How do you choose the optimal tree depth?\n",
    "\n",
    "\n",
    "* By performing hyperparameter tunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605dfae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1f63be5",
   "metadata": {},
   "source": [
    "### Q) Decision tree work on if else condition then why we don't use rule based programming(if-else conditions in python) instead of Decision Tree? \n",
    "\n",
    "\n",
    "* Ruled based programming is not efficiently work on large set of data.\n",
    "\n",
    "\n",
    "* If there's a minor change in data will happen we need to update the program again but decision tree handle changes in data by self.\n",
    "\n",
    "\n",
    "* Decision tree learn by self and don't require human interactions but ruled-based programming requires human interaction to set conditions and rules in program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db3b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d1235a2",
   "metadata": {},
   "source": [
    "### Q) What is sub-tree?\n",
    "\n",
    "\n",
    "* It is small part or subset of decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78d121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24984df2",
   "metadata": {},
   "source": [
    "### Q) What is splitting point in decision tree?\n",
    "\n",
    "\n",
    "* From where the node splits is known as splitting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a0c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0d19f36",
   "metadata": {},
   "source": [
    "### Q) How do you decide which split criterion to use in a decision tree?\n",
    "\n",
    "\n",
    "* We do trail and error then which splitting criterion gives better result choose it or perform hyperparameter tunning to select best splitting criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fdda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c02e9576",
   "metadata": {},
   "source": [
    "### Q) What is pure node?\n",
    "\n",
    "\n",
    "* Pure node means node which contain observations of same class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522bacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ade033f",
   "metadata": {},
   "source": [
    "### Q) Which is best criterion entropy or gini and which is fast to compute?\n",
    "\n",
    "\n",
    "* Best criterion depends on the problem statement and given dataset and Gini is computational faster than entropy due to its simple calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8f0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30a62780",
   "metadata": {},
   "source": [
    "### Q) What is entrophy? \n",
    "\n",
    "\n",
    "* Entropy is a measure of the impurity or uncertainty.\n",
    "\n",
    "\n",
    "* The value of entropy ranges between 0-1. The lower entropy value indicates low impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c32ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b1b08a4",
   "metadata": {},
   "source": [
    "### Q) Find the entropy of 30 Yes and 70 No.\n",
    "\n",
    "    Entropy = -Σ Pi * log2(Pi)\n",
    "\n",
    "    Pi = Probability of category\n",
    "\n",
    "    Entropy = -p(x)*log2(p(x)) - p(y)*log2(p(y))\n",
    "    \n",
    "    Entropy = -0.3*log2(0.3) - 0.7*log2(0.7)\n",
    "\n",
    "    Entropy = 0.98 \n",
    "\n",
    "    p(x) is the probability of a Yes response\n",
    "    p(y) is the probability of a No response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc1486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39e660f5",
   "metadata": {},
   "source": [
    "### Q) What is gini?\n",
    "\n",
    "\n",
    "* Gini is measure of the impurity or uncertainty.\n",
    "\n",
    "\n",
    "* Same as entropy the value of gini ranges between 0-1. The lower gini value indicates the low inpurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2128e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "297861b3",
   "metadata": {},
   "source": [
    "### Q) Find Gini for 30 Yes and 70 No.\n",
    "\n",
    "\n",
    "    Gini = 1-(Py² + Pn²)\n",
    "\n",
    "    Py = Probability of Yes\n",
    "    Pn = Probability of No\n",
    "    \n",
    "    Gini = 1-((0.3)² + (0.7)²)\n",
    "    \n",
    "    Gini = 0.769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894cf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dee501d5",
   "metadata": {},
   "source": [
    "### Q) Uses of Entropy and gini?\n",
    "\n",
    "\n",
    "Entropy and gini measures the  impurity, which will helps in selecting the feature for node and their category for splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bbebde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c06bbe49",
   "metadata": {},
   "source": [
    "### Q) What is the difference between Gini and entropy?\n",
    "\n",
    "* Gini and entrophy are same, they both measures the impurity or uncertainty in data. The only difference is the way they calculate impurity, it means their formulas are different.\n",
    "\n",
    "\n",
    "\n",
    "* The formula of entropy if -Σ Pi * log2(Pi).\n",
    "    \n",
    "\n",
    "* The formula of gini is 1-(Py²+Pn²).\n",
    "\n",
    "\n",
    "* Gini is computational fast than entropy due to its simple formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3245a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b40dee9",
   "metadata": {},
   "source": [
    "### Q) What is information gain and it's use?\n",
    "\n",
    "* Information gain measure that after a split how much information of data will keep by the feature and how much a feature contribute to the target variable.\n",
    "\n",
    "\n",
    "* Information gain is used to select the features for nodes in decision tree.\n",
    "\n",
    "\n",
    "* Information gain is also used for feature selection as it give features importance ranking for all variables.\n",
    "\n",
    "\n",
    "* Information gain value ranges between 0-1, value closer indiates that feature provides more information about the target variable.\n",
    "\n",
    "\n",
    "\n",
    "      Information Gain = Entropy(parent) - [Weighted Average * Entropy(children)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82134924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5139fa9",
   "metadata": {},
   "source": [
    "### Q) How to calculate information gain?\n",
    "\n",
    "\n",
    "* The formula for information gain based on entropy is: Information Gain = Entropy(parent) - [weighted average * Entropy(children)]\n",
    "\n",
    "\n",
    "* The formula for information gain based on Gini impurity is: Information Gain = Gini(parent) - [weighted average * Gini(children)]\n",
    "\n",
    "\n",
    "Where:-\n",
    "\n",
    "\n",
    "* Entropy(parent) = Entropy value of target variable. \n",
    "* Weightage average = sum of Category1 of independent variable / Total number of observations\n",
    "* Entropy(children) = Entropy of category of independent variable.\n",
    "\n",
    "We have independent variable climate which consists of (rain, hot, cold) and dependent variable child play (Yes/No).\n",
    "\n",
    "\n",
    "* Information gain = Entropy(child play) - [sum of rain / total no. of observations * Entropy(rain) + sum of hot / total no. of observations * Entropy(hot) + sum of cold / total no. of observations * Entropy(cold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f4947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd3640b",
   "metadata": {},
   "source": [
    "### Q) Difference between Regression models and decision tree?\n",
    "\n",
    "* Regression models works by building a linear relationship between independent variables and dependent variable while decision tree do not build any relationship between variables.\n",
    "\n",
    "\n",
    "* Decision Tree occurs to overfitting more easily than regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0868f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99b556b4",
   "metadata": {},
   "source": [
    "### Q) What is the impact of outliers in decision tree?\n",
    "\n",
    "\n",
    "* Outliers can have a significant impact on decision trees. Decision tree works by calculating the entropy or gini for each node then splits it into binary branches, if outliers are present it will increase the value of entropy.\n",
    "\n",
    "\n",
    "There are several approaches to deal with outliers:-\n",
    "\n",
    "\n",
    "* Remove outliers.\n",
    "\n",
    "\n",
    "* Use ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f7097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da68c3e5",
   "metadata": {},
   "source": [
    "### Q) Why we use regression tree and when?\n",
    "\n",
    "\n",
    "* It is used to predict the continous numerical value and when the data is non-linear then linear regression do not perform well, in that case we use regression trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d9d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6b1c529",
   "metadata": {},
   "source": [
    "### Q) How do regression trees handle missing values in the data?\n",
    "\n",
    "\n",
    "* Regression trees can handle missing values by using dopping the observations with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa92a0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca45433d",
   "metadata": {},
   "source": [
    "### Q) Can regression trees handle categorical variables in the data? If yes, how?\n",
    "\n",
    "\n",
    "* No, regression trees can not  handle categorical variables directly. First, we have to encode categorical variables by using any encoding technique and then make regression tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039db50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "992f7a28",
   "metadata": {},
   "source": [
    "### Q) What is the role of pruning in regression trees?\n",
    "\n",
    "\n",
    "* Pruning is used to prevent regression trees from overfitting. Prunning removes the branches and decision nodes of regression tree which do not contribute to the performance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a67375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbd119e5",
   "metadata": {},
   "source": [
    "### Q) Can regression trees handle multicollinearity in the data?\n",
    "\n",
    "\n",
    "* Yes, regression trees handle multicollinearity in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74dc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12c492e0",
   "metadata": {},
   "source": [
    "### Q) How does the depth of a regression tree affect its performance?\n",
    "\n",
    "\n",
    "* When depth of the decision tree is more then the chances of overfitting will be more, because in that case decision tree will captures the complex relationships and pattern of trainning data too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af10022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a485b631",
   "metadata": {},
   "source": [
    "### Q) How do you handle overfitting in a regression tree?\n",
    "\n",
    "\n",
    "* Overfitting can be handled in a regression tree by using pruning techniques and hyperparameter tunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce3977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ad09665",
   "metadata": {},
   "source": [
    "### Q) How do you evaluate the performance of a regression tree model?\n",
    "\n",
    "\n",
    "* The performance of a regression tree model can be evaluated by using metrics such as mean squared error, RMSE, mean absolute error, R-squared, adjusted R squared or cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283de65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0b02a12",
   "metadata": {},
   "source": [
    "Q41) How regression tree works and find their nodes and condition?\n",
    "\n",
    "\n",
    "* Regression Tree will try all features as a root node and their possible values as a threshold value(condition) and split it into binary branches.\n",
    "\n",
    "\n",
    "* Then it will calculate variance with the help of the Mean Squared Error formula for root node and for the values of dependent variable which each branch contains.\n",
    "\n",
    "\n",
    "*  Then calculate the Mean Squared Error Reduction(variance reduction) for each possible splits according to the data.\n",
    "\n",
    "\n",
    "* This process of calculating Mean Squared Error and Mean Squared Error Reduction(Variance Reduction) is done for all possible splits that are the combination of features and their values.\n",
    "\n",
    "\n",
    "* Then it will compare all Mean Squared Error Reduction(variance reduction) values and take that feature as a root node and their value as a threshold value which contains the maximum Mean Squared Error Reduction(variance reduction) value and split according that.\n",
    "\n",
    "\n",
    "* If any branch produces the pure node which carries only one value of dependent variable it means it is our leaf node.\n",
    "\n",
    "\n",
    "* In case of pre-prunning and post-prunning a leaf node contains multiple values of dependent variable so, in this case it will take the mean of all values and give as a output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113e5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3c9521b",
   "metadata": {},
   "source": [
    "### Q) Why we choose splitting parameter as random or best?\n",
    "\n",
    "* Perform trail and error which splitting parameter gives better performance of model choose that splitting parameter or perform hyperparameter tunning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f14f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6da26604",
   "metadata": {},
   "source": [
    "### Q) What is CART?\n",
    "\n",
    "\n",
    "* CART (Classification and Regression Trees) is a popular algorithm used in machine learning for creating decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b83834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
