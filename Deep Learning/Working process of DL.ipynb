{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858c6492",
   "metadata": {},
   "source": [
    "### Which weight initializer is used when?\n",
    "\n",
    "* **Xavier/Glorot Initialization**: Used when we use Sigmoid or Tanh Activation Function.\n",
    "\n",
    "\n",
    "* **He Initialization**: Used with ReLU.\n",
    "\n",
    "We have 2 types of both normal and uniform. Perform trail and test and then select better one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0f410",
   "metadata": {},
   "source": [
    "* Proper weight initialization can help the neural network learn from the data in a more efficient way. \n",
    "\n",
    "\n",
    "* There are many different weight initialization techniques available. The best technique to use depends on the specific task and the architecture of the ANN.\n",
    "\n",
    "\n",
    "* It is important to experiment with different weight initialization techniques to see which one works best for your particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1278ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82b541e5",
   "metadata": {},
   "source": [
    "### Which Activation Function is to apply in which layer and when?\n",
    "\n",
    "* **Regression**: ReLU + Linear\n",
    "\n",
    "\n",
    "* **Binary Classification**: ReLU + Sigmoid\n",
    "\n",
    "\n",
    "* **Multi-Class Classfication**: ReLU + Softmax\n",
    "\n",
    "\n",
    "**`Note`**: There is no such rule that always always these comobinations. Perform trail and error to choose better combination than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc957493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5ddccce",
   "metadata": {},
   "source": [
    "### Q) Batch normalization is used ANN and when with code and best practice to apply batch normalization?\n",
    "\n",
    "Yes, batch normalization can be used in Artificial Neural Networks (ANNs). In fact, batch normalization was originally introduced for feedforward ANNs and has since been widely adopted in various neural network architectures.\n",
    "\n",
    "**Steps**:-\n",
    "\n",
    "1) Input data (features) are fed into the layer.\n",
    "\n",
    "\n",
    "2) The layer applies a linear transformation (weights and biases) to the input data.\n",
    "\n",
    "\n",
    "3) Batch Normalization is applied.\n",
    "\n",
    "\n",
    "4) The output of the linear transformation is passed through an activation function, producing the activations of the layer.\n",
    "\n",
    "\n",
    "**`Best Practice`**:-\n",
    "\n",
    "**Apply Batch Normalization after Every Hidden Layer (or Almost All)**\n",
    "\n",
    "**Avoid Batch Normalization in the Output Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f1ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=(input_dim,)),\n",
    "    tf.keras.layers.BatchNormalization(), \n",
    "    tf.keras.layers.Activation('relu'),    \n",
    "    \n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.BatchNormalization(), \n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468a806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90c4ffaf",
   "metadata": {},
   "source": [
    "### Which optimizer is best in which case?\n",
    "\n",
    "90% time we are using 'Adam' Optimizer in all problem statements. Perform trail and error to choose better optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492f301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfab0ca1",
   "metadata": {},
   "source": [
    "### Q) How to specify number of neuron in each layer?\n",
    "\n",
    "**Input Layer**: Number of neurons are equal to number of independent feataures in dataset.\n",
    "\n",
    "\n",
    "**Hidden Layer**: In first hidden layer number of neurons are equal to neurons in input layer and second hidden layer neurons are half of first hidden layer and like this. But their is no such rule that always apply this technique perform trail and error also.\n",
    "\n",
    "\n",
    "**Output Layer**: In case of regression and binary classification number of neurons in output layer is one and in case of multi-class classification and image data number of neurons in output layer is depennd on number of categories in dependent variabl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09fa37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fdf2193",
   "metadata": {},
   "source": [
    "### Calculate Loss by which loss function?\n",
    "\n",
    "* **Regression**: MSE, MAE, Huber Loss\n",
    "    \n",
    "    \n",
    "* **Binary Classification**: Binary Cross Entropy\n",
    "\n",
    "\n",
    "* **Multi-Class Classfication**: Categorical Loss Entopy\n",
    "\n",
    "\n",
    "* **Image Data**: Sparse Categorical Entopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e2713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe1fa286",
   "metadata": {},
   "source": [
    "### Good Practice for Dropout percentage.\n",
    "\n",
    "\n",
    "**Smaller layers**: Use a lower dropout percentage, such as 0.1 or 0.2.\n",
    "\n",
    "\n",
    "**Larger layers**: Use a higher dropout percentage, such as 0.3 or 0.4.\n",
    "\n",
    "\n",
    "**More complex tasks**: Use a higher dropout percentage.\n",
    "\n",
    "\n",
    "**Less training data**: Use a higher dropout percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f046f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b58577",
   "metadata": {},
   "source": [
    "### `Notes`:\n",
    "\n",
    "* While working on image data we first scale matrix of data. We can scale manually and we have libraries also then we convert matrix of image data into 1D array by using .reshape function of numpy and then build neural network of it.\n",
    "\n",
    "\n",
    "* After neural network is build we evaulate the performance of neural network on same metrics we use in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2a312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d191612",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fad728",
   "metadata": {},
   "source": [
    "* **Stride is only apply on pooling layer not on convolutional layer becuase we in convolutional layer we want to extract more primitive features as possible but in pooling we only want high-level primitive features.**\n",
    "\n",
    "\n",
    "* **In CNN we apply batch normalization in convolutional layer only, not in pooling layer and fully connected layers.**\n",
    "\n",
    "\n",
    "* **In CNN Loss is calculated by binarycrossentropy, categorical crossentropy and sparse categorical crossentropy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b9437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07e316ae",
   "metadata": {},
   "source": [
    "### Q) What is Batch Normalization in CNN and when to apply?\n",
    "\n",
    "* The purpose of batch normalization in CNNs is to stabilize and speed up the training process by normalizing the input data within each mini-batch(subset of data) during training. This helps the model converge faster(minimize loss faster) and improves overall performance.\n",
    "\n",
    "\n",
    "* Batch normalization (BN) is a technique used to improve the training of deep neural networks. It is typically used after convolutional layers and before activation functions.\n",
    "\n",
    "\n",
    "* Batch normalization works by normalizing the activations of a layer across a batch of inputs. This helps to stabilize the training process and prevents the activations from becoming too large or too small. BN can also help to improve the generalization performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='valid', input_shape=(256, 256, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51620e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbee5206",
   "metadata": {},
   "source": [
    "### Activations functions are only applied after convolutional layer not on pooling layer.\n",
    "\n",
    "**Activation Function after Convolutional Layer**:\n",
    "After applying convolution to the input data using a set of filters (kernels), the resulting feature maps go through an activation function element-wise. The activation function introduces non-linearity to the output of the convolutional layer, which allows the network to learn and model complex relationships in the data. Common activation functions used in CNNs include ReLU (Rectified Linear Unit), Leaky ReLU, ELU (Exponential Linear Unit), and others.\n",
    "\n",
    "**No Activation Function after Pooling Layer**:\n",
    "After applying pooling (e.g., MaxPooling or AveragePooling), there is no activation function applied directly to the pooled feature maps. Pooling layers are purely for down-sampling and spatial dimension reduction, and they do not introduce non-linearity.\n",
    "\n",
    "The typical CNN architecture sequence involving convolutional layers, activation functions, and pooling layers is as follows:\n",
    "\n",
    "**Input Data -> Convolutional Layer -> Activation Function -> Pooling Layer ->..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abea6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "293b4d56",
   "metadata": {},
   "source": [
    "### Q) When to apply padding?\n",
    "\n",
    "Padding is applied to the input data before performing convolutional operations in a Convolutional Neural Network (CNN). The primary purpose of padding is to maintain the spatial dimensions of the input and output feature maps during convolution.\n",
    "\n",
    "\n",
    "Do not apply padding on pooling layer because we want only high-level primitive features in pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3e51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e015df1f",
   "metadata": {},
   "source": [
    "### Dropout percentages\n",
    "\n",
    "**You can use dropout in both convolutional layers and pooling layers in CNNs.**\n",
    "\n",
    "**Good Practice in convolutional layers and pooling layers**\n",
    "\n",
    "In **pooling layers**, dropout is not typically used. This is because pooling layers are used to reduce the size of the feature maps, and dropout would introduce additional complexity to the model. However, some researchers have experimented with using dropout in pooling layers, and some have found that it can improve the performance of the model.\n",
    "\n",
    "\n",
    "**Here are some guidelines for using dropout in CNNs:**\n",
    "\n",
    "\n",
    "**Convolutional layers**: Dropout can be used in convolutional layers to prevent overfitting. A good starting point is to use a dropout percentage of 0.2 or 0.3.\n",
    "\n",
    "\n",
    "**Pooling layers**: Dropout is not typically used in pooling layers.\n",
    "\n",
    "\n",
    "**Fully connected layers**: Dropout can be used in fully connected layers to prevent overfitting. A good starting point is to use a dropout percentage of 0.5 or 0.6.\n",
    "\n",
    "----------------------------------------------------\n",
    "\n",
    "**Good Practice for neural networks**\n",
    "\n",
    "\n",
    "**Smaller layers**: Use a lower dropout percentage, such as 0.1 or 0.2.\n",
    "\n",
    "\n",
    "**Larger layers**: Use a higher dropout percentage, such as 0.3 or 0.4.\n",
    "\n",
    "\n",
    "**More complex tasks**: Use a higher dropout percentage.\n",
    "\n",
    "\n",
    "**Less training data**: Use a higher dropout percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115e5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d915826d",
   "metadata": {},
   "source": [
    "### Which optimizer, loss function and metric to use in CNN:- \n",
    "\n",
    "* Use a binary crossentropy loss function, which is a good choice for binary classification tasks and categorical crossentrophy for multi-class classification tasks.\n",
    "\n",
    "\n",
    "* Use the Adam optimizer, which is a good choice for most deep learning tasks.\n",
    "\n",
    "\n",
    "* Use the accuracy metric, which is a good choice for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1dbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8668072c",
   "metadata": {},
   "source": [
    "### Perfect Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', input_shape=(256, 256, 3))) # for RGB Image\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d821672",
   "metadata": {},
   "source": [
    "**Add object of early stopping parameters here then continue with below code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data and validate on the validation data\n",
    "history = model.fit(train_ds, epochs=20, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd2d29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a23e742",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Q) How to choose appropriate number of filters at every convolutional layer.\n",
    "\n",
    "\n",
    "A common practice is to start with a small number of filters in the first layer and gradually increase the number of filters in subsequent layers. For example, in a basic CNN architecture, the number of filters might be set as follows:\n",
    "\n",
    "**`First Convolutional Layer`**: Fewer filters, e.g., 32 or 64.\n",
    "\n",
    "\n",
    "**`Intermediate Convolutional Layers`**: Increasing number of filters, e.g., 128, 256, etc.\n",
    "\n",
    "\n",
    "**`Final Convolutional Layer`**: The number of filters can be set to the number of classes in the classification task (for classification problems).\n",
    "\n",
    "\n",
    "The choice of the number of filters can also depend on the available computational resources and the size of the dataset. Larger numbers of filters increase the model's capacity to learn intricate patterns but also increase the computational cost and the risk of overfitting if the dataset is small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8088e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39e70bc6",
   "metadata": {},
   "source": [
    "### Q) How to specify correct input shape of images while creating dataset?\n",
    "\n",
    "\n",
    "The choice of the image size (image_size) when creating a dataset of images depends on various factors, including the characteristics of the dataset, the available computational resources, and the requirements of the specific deep learning model or task you are working on.\n",
    "\n",
    "\n",
    "As a starting point, image sizes such as (256, 256) or (224, 224) are commonly used in various computer vision tasks. These sizes are often chosen because they strike a balance between maintaining relevant details in the images and computational efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2421e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "552b9b63",
   "metadata": {},
   "source": [
    "### Q) What will happen if we incease the number of convolutional layers?\n",
    "\n",
    "\n",
    "Adding more convolutional layers can make the model more complex and difficult to train, but it can also improve the model's performance by extracting more features and reducing overfitting.\n",
    "\n",
    "\n",
    "**`Increased complexity`**: Adding more convolutional layers will increase the complexity of the CNN model. This can make the model more difficult to train, but it can also improve the model's performance.\n",
    "\n",
    "\n",
    "**`Improved feature extraction`**: More convolutional layers can extract more features from the input data. This can improve the model's ability to recognize patterns in the data and make more accurate predictions.\n",
    "\n",
    "\n",
    "**`Reduced overfitting`**: Adding more convolutional layers can help to reduce overfitting. This is because the model will be able to learn more features from the data, which will make it less likely to memorize the training data and generalize better to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20286b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b58c0e3d",
   "metadata": {},
   "source": [
    "### `'mini-batch'` parameter\n",
    "\n",
    "The size of the mini-batch is a hyperparameter that can be tuned to improve the performance of the model. A smaller mini-batch size can help to prevent overfitting, but it can also make the training process slower. A larger mini-batch size can speed up the training process, but it can also increase the risk of overfitting.\n",
    "\n",
    "The optimal mini-batch size depends on the specific problem and the dataset. However, a good starting point is a mini-batch size of 32 to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531faab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
