{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858c6492",
   "metadata": {},
   "source": [
    "### Which weight initializer is used when?\n",
    "\n",
    "* **Xavier/Glorot Initialization**: Used when we use Sigmoid or Tanh Activation Function.\n",
    "\n",
    "\n",
    "* **He Initialization**: Used with ReLU.\n",
    "\n",
    "We have 2 types of both `normal` and `uniform`. Perform trail and test and then select better one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0f410",
   "metadata": {},
   "source": [
    "* Weight initialization is important for both the input and hidden layers of a neural network. Proper weight initialization helps in improving the convergence speed and stability of the training process. \n",
    "\n",
    "\n",
    "* Weight initializers are used in all layers of a neural network, not just the input layer. The best practice for weight initialization depends on the activation function used in the neural network.\n",
    "\n",
    "\n",
    "* Their is no such rule these weight initalizers are used with only these activation functions. Do experiments, trail and test, it is `important` in real time.\n",
    "\n",
    "\n",
    "* Proper weight initialization can help the neural network learn from the data in a more efficient way. \n",
    "\n",
    "\n",
    "* There are many different weight initialization techniques available. The best technique to use depends on the specific task and the architecture of the ANN.\n",
    "\n",
    "\n",
    "* It is important to experiment with different weight initialization techniques to see which one works best for your particular task.\n",
    "\n",
    "\n",
    "* For neural networks with ReLU activation functions, it is common to use a He initialization.\n",
    "\n",
    "\n",
    "* He initialization ensures that the weights are initialized in a way that allows the neural network to learn more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fefa6",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "\n",
    "\n",
    "* We can use weight initializers in both pooling and convolutional layers in CNNs. In fact, it is important to use a weight initializer in all layers of a CNN, as it can help to improve the convergence of the network and prevent it from overfitting.\n",
    "\n",
    "\n",
    "* We can use weight initializers in every hidden layer of an ANN. In fact, it is generally recommended to use a weight initializer in all layers of an ANN, as it can help to improve the convergence of the network and prevent it from overfitting.\n",
    "\n",
    "\n",
    "* Whether to use the same weight initializer in every layer of a CNN or ANN, or to use different initializers at each layer, is a matter of debate. There is no single answer that is universally correct, as the best approach will depend on the specific dataset and task that the network is being trained on.\n",
    "\n",
    "\n",
    "* If the network is very deep, it may be beneficial to use different initializers at different layers. This is because the deeper layers of the network will need to learn more complex features, and using a different initializer can help to prevent the network from becoming too unstable.\n",
    "\n",
    "\n",
    "* If the network is not very deep, it may be sufficient to use the same initializer in every layer. This is because the shallower layers of the network will not need to learn as complex features, and using the same initializer can help to ensure that the network converges more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1278ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82b541e5",
   "metadata": {},
   "source": [
    "### Which Activation Function is to apply in which layer and when?\n",
    "\n",
    "* **Regression**: ReLU + Linear\n",
    "\n",
    "\n",
    "* **Binary Classification**: ReLU + Sigmoid\n",
    "\n",
    "\n",
    "* **Multi-Class Classfication**: ReLU + Softmax\n",
    "\n",
    "\n",
    "**`Note`**: There is no such rule that always always these comobinations. Perform trail and error to choose better combination than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc957493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5ddccce",
   "metadata": {},
   "source": [
    "### Q) Batch normalization is used ANN and when with code and best practice to apply batch normalization?\n",
    "\n",
    "Yes, batch normalization can be used in Artificial Neural Networks (ANNs). In fact, batch normalization was originally introduced for feedforward ANNs and has since been widely adopted in various neural network architectures.\n",
    "\n",
    "* When we apply batch normalization the values of input are converted in the range of (0-1) and then same activation function is applied on specific layer so output of all neurons are almost similar, so neural learn that output in more efficient way that's why we use BN.\n",
    "\n",
    "\n",
    "* If we use BN after activation function so output will be converted in the range of 0-1 that will create the problem of vanishing gradient descent, this happens same with sigmoid in 1990's. \n",
    "\n",
    "\n",
    "**Steps**:-\n",
    "\n",
    "1) Input data (features) are fed into the layer.\n",
    "\n",
    "\n",
    "2) The layer applies a linear transformation (weights and biases) to the input data.\n",
    "\n",
    "\n",
    "3) Batch Normalization is applied.\n",
    "\n",
    "\n",
    "4) The output of the linear transformation is passed through an activation function, producing the activations of the layer.\n",
    "\n",
    "\n",
    "**`Best Practice`**:-\n",
    "\n",
    "**Apply Batch Normalization after Every Hidden Layer (or Almost All)**\n",
    "\n",
    "**Avoid Batch Normalization in the Output Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f1ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=(input_dim,)),\n",
    "    tf.keras.layers.BatchNormalization(), \n",
    "    tf.keras.layers.Activation('relu'),    \n",
    "    \n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.BatchNormalization(), \n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468a806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90c4ffaf",
   "metadata": {},
   "source": [
    "### Which optimizer is best in which case?\n",
    "\n",
    "90% time we are using 'Adam' Optimizer in all problem statements. Perform trail and error to choose better optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492f301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfab0ca1",
   "metadata": {},
   "source": [
    "### Q) How to specify number of neuron in each layer?\n",
    "\n",
    "**Input Layer**: Number of neurons are equal to number of independent feataures in dataset.\n",
    "\n",
    "\n",
    "**Hidden Layer**: In first hidden layer number of neurons are equal to neurons in input layer and second hidden layer neurons are half of first hidden layer and like this. But their is no such rule that always apply this technique perform trail and error also.\n",
    "\n",
    "\n",
    "* The number of neurons in the hidden layers should be between the number of neurons in the input layer and the number of neurons in the output layer.\n",
    "\n",
    "\n",
    "**Output Layer**: In case of regression and binary classification number of neurons in output layer is one and in case of multi-class classification and image data number of neurons in output layer is depennd on number of categories in dependent variabl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09fa37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fdf2193",
   "metadata": {},
   "source": [
    "### Calculate Loss by which loss function?\n",
    "\n",
    "* **Regression**: MSE, MAE, Huber Loss\n",
    "    \n",
    "    \n",
    "* **Binary Classification**: Binary Cross Entropy\n",
    "\n",
    "\n",
    "* **Multi-Class Classfication**: Categorical Loss Entopy\n",
    "\n",
    "\n",
    "* **Image Data**: Sparse Categorical Entopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e2713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe1fa286",
   "metadata": {},
   "source": [
    "### Good Practice for Dropout percentage.\n",
    "\n",
    "\n",
    "**Smaller layers**: Use a lower dropout percentage, such as 0.1 or 0.2.\n",
    "\n",
    "\n",
    "**Larger layers**: Use a higher dropout percentage, such as 0.3 or 0.4.\n",
    "\n",
    "\n",
    "**More complex tasks**: Use a higher dropout percentage.\n",
    "\n",
    "\n",
    "**Less training data**: Use a higher dropout percentage.\n",
    "\n",
    "\n",
    "* Dropout regularization can be used in both the input and hidden layers of a neural network. However, it is more commonly used in the hidden layers. This is because the input layer is typically not as prone to overfitting as the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f046f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7d92a8d",
   "metadata": {},
   "source": [
    "## Batch_size parameter\n",
    "\n",
    "* The batch_size parameter in deep learning represents the number of training examples utilized in one forward/backward pass.\n",
    "\n",
    "\n",
    "* A larger batch size can improve the performance of the neural network, but it can also make the training process slower.\n",
    "\n",
    "\n",
    "* A smaller batch size can make the training process faster, but it can also lead to less accurate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adaf7db",
   "metadata": {},
   "source": [
    "**Small Batch Size**:\n",
    "\n",
    "\n",
    "**Advantages**: Smaller batch sizes tend to converge faster as they update the model's parameters more frequently. They also require less memory, which can be beneficial when working with limited resources.\n",
    "\n",
    "\n",
    "**Disadvantages**: Smaller batch sizes can introduce more noise to the parameter updates, potentially leading to less stable convergence and noisy gradients.\n",
    "\n",
    "\n",
    "**Large Batch Size**:\n",
    "\n",
    "\n",
    "**Advantages**: Larger batch sizes can provide smoother gradient updates, potentially leading to more stable convergence. They can also benefit from vectorized operations, improving computational efficiency.\n",
    "\n",
    "\n",
    "**Disadvantages**: Larger batch sizes require more memory and may take longer per epoch due to fewer parameter updates. They might not generalize as well to the validation set due to their potential to get stuck in local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ac387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b58577",
   "metadata": {},
   "source": [
    "### `Notes`:\n",
    "\n",
    "* While working on image data we first scale matrix of data. We can scale manually and we have libraries also then we convert matrix of image data into 1D array by using .reshape function of numpy and then build neural network of it.\n",
    "\n",
    "\n",
    "* After neural network is build we evaulate the performance of neural network on same metrics we use in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2a312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d191612",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fad728",
   "metadata": {},
   "source": [
    "* **Stride is only apply on pooling layer not on convolutional layer becuase we in convolutional layer we want to extract more primitive features as possible but in pooling we only want high-level primitive features.**\n",
    "\n",
    "\n",
    "* **By using strides devploy model gets faster in prediction but we loss information by using strides.** \n",
    "\n",
    "\n",
    "* **In CNN we apply batch normalization in convolutional layer only, not in pooling layer and fully connected layers.**\n",
    "\n",
    "\n",
    "* **In CNN Loss is calculated by binarycrossentropy, categorical crossentropy and sparse categorical crossentropy.**\n",
    "\n",
    "\n",
    "* **Pooling layers are used to reduce the dimensionality of the feature maps output by the convolutional layers, while activation functions are used to introduce non-linearity into the network. Non-linearity is important for deep learning models because it allows them to learn more complex patterns. However, pooling layers are already non-linear, so there is no need to add an activation function after them.**\n",
    "\n",
    "\n",
    "* **When strides are used, more information is lost from the input data, which can lead to a slight increase in the error rate. However, the advantage of using strides is that it can speed up the training process and the processing speed of the application after deployment.**\n",
    "\n",
    "\n",
    "* **Data augmentation is a technique that can be used to artificially increase the size of a training dataset by creating modified copies of images in the dataset. This can help to prevent overfitting, which is a problem that can occur when a model is trained on a small dataset.**\n",
    "\n",
    "\n",
    "* **Data augmentation Increases training data, Reduces overfitting and improves performance of model.**\n",
    "\n",
    "\n",
    "* **In real time projects image sizes are big. So, we use pooling almost in every project.**\n",
    "\n",
    "\n",
    "* **We use strides in the pooling layer only because our goal is to extract high-level primitive features from the input. Strides result in loss of information, so we only use them in the pooling layer and not in the convolutional layer.**\n",
    "\n",
    "* **We can use weight initializers in both pooling and convolutional layers in CNNs. In fact, it is important to use a weight initializer in all layers of a CNN, as it can help to improve the convergence of the network and prevent it from overfitting.**\n",
    "\n",
    "\n",
    "* **We can use weight initializers in every hidden layer of an ANN. In fact, it is generally recommended to use a weight initializer in all layers of an ANN, as it can help to improve the convergence of the network and prevent it from overfitting.**\n",
    "\n",
    "\n",
    "* **Whether to use the same weight initializer in every layer of a CNN or ANN, or to use different initializers at each layer, is a matter of debate. There is no single answer that is universally correct, as the best approach will depend on the specific dataset and task that the network is being trained on.**\n",
    "\n",
    "\n",
    "* **If the network is very deep, it may be beneficial to use different initializers at different layers. This is because the deeper layers of the network will need to learn more complex features, and using a different initializer can help to prevent the network from becoming too unstable.**\n",
    "\n",
    "\n",
    "* **If the network is not very deep, it may be sufficient to use the same initializer in every layer. This is because the shallower layers of the network will not need to learn as complex features, and using the same initializer can help to ensure that the network converges more quickly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b9437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf8faf07",
   "metadata": {},
   "source": [
    "### Q) USe of batch_size parameter?\n",
    "\n",
    "Batch size refers to the number of training examples used in one iteration of the training process in a CNN. It is important because it determines how many samples are processed together in each step during training. Using a proper batch size allows efficient utilization of computational resources and can lead to more stable and faster convergence during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68f5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07e316ae",
   "metadata": {},
   "source": [
    "### Q) What is Batch Normalization in CNN and when to apply?\n",
    "\n",
    "* The purpose of batch normalization in CNNs is to stabilize and speed up the training process by normalizing the input data within each mini-batch(subset of data) during training. This helps the model converge faster(minimize loss faster) and improves overall performance.\n",
    "\n",
    "\n",
    "* Batch normalization (BN) is a technique used to improve the training of deep neural networks. It is typically used after convolutional layers and before activation functions.\n",
    "\n",
    "\n",
    "* Batch normalization works by normalizing the activations of a layer across a batch of inputs. This helps to stabilize the training process and prevents the activations from becoming too large or too small. BN can also help to improve the generalization performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='valid', input_shape=(256, 256, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f434ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f4b682",
   "metadata": {},
   "source": [
    "### Q) Why we batch normalization only after convolutional layer and not after pooling layer?\n",
    "\n",
    "We use batch normalization after convolutional layers because it helps make the training process more stable and faster. Batch normalization normalizes the inputs to each layer, making the optimization process more efficient. Pooling layers are used for downsampling and do not require batch normalization for their specific task. Therefore, we apply batch normalization only after convolutional layers to improve the performance of the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803fb47d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7088b72a",
   "metadata": {},
   "source": [
    "### Q) Why do we apply the activation function after batch normalization instead of before?\n",
    "\n",
    "* We apply the activation function after batch normalization to prevent the vanishing gradient problem and ensure stable and faster training. Batch normalization normalizes the inputs, making them suitable for the activation function, leading to more effective learning.\n",
    "\n",
    "\n",
    "* Applying batch normalization before the activation function scales the output of mini-batches, making the model learn efficiently. It also speeds up the training process, and the activation function performs effectively on the normalized output of batch normalization.\n",
    "\n",
    "\n",
    "* If we apply batch normalization after the activation function, it will normalize the output values to a range between 0 and 1. This can help to prevent the vanishing gradient problem, which can occur with activation functions such as the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51620e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbee5206",
   "metadata": {},
   "source": [
    "### Activations functions are only applied after convolutional layer not on pooling layer.\n",
    "\n",
    "**Activation Function after Convolutional Layer**:\n",
    "After applying convolution to the input data using a set of filters (kernels), the resulting feature maps go through an activation function element-wise. The activation function introduces non-linearity to the output of the convolutional layer, which allows the network to learn and model complex relationships in the data. Common activation functions used in CNNs include ReLU (Rectified Linear Unit), Leaky ReLU, ELU (Exponential Linear Unit), and others.\n",
    "\n",
    "**No Activation Function after Pooling Layer**:\n",
    "After applying pooling (e.g., MaxPooling or AveragePooling), there is no activation function applied directly to the pooled feature maps. Pooling layers are purely for down-sampling and spatial dimension reduction, and they do not introduce non-linearity.\n",
    "\n",
    "The typical CNN architecture sequence involving convolutional layers, activation functions, and pooling layers is as follows:\n",
    "\n",
    "**Input Data -> Convolutional Layer -> Activation Function -> Pooling Layer ->..**\n",
    "\n",
    "### `IMPORTANT`\n",
    "\n",
    "**Pooling layers are used to reduce the dimensionality of the feature maps output by the convolutional layers, while activation functions are used to introduce non-linearity into the network. Non-linearity is important for deep learning models because it allows them to learn more complex patterns. However, pooling layers are already non-linear, so there is no need to add an activation function after them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abea6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "293b4d56",
   "metadata": {},
   "source": [
    "### Q) When to apply padding?\n",
    "\n",
    "* Padding is applied to the input data before performing convolutional operations in a Convolutional Neural Network (CNN). The primary purpose of padding is to maintain the spatial dimensions of the input and output feature maps during convolution.\n",
    "\n",
    "\n",
    "* Do not apply padding on pooling layer because we want only high-level primitive features in pooling layer.\n",
    "\n",
    "\n",
    "* In industry, zero padding is usually used, but there are other padding methods as well, such as making the outer layer the same as the nearby grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3e51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e015df1f",
   "metadata": {},
   "source": [
    "### Dropout percentages\n",
    "\n",
    "**You can use dropout in both convolutional layers and pooling layers in CNNs.**\n",
    "\n",
    "**Good Practice in convolutional layers and pooling layers**\n",
    "\n",
    "In **pooling layers**, dropout is not typically used. This is because pooling layers are used to reduce the size of the feature maps, and dropout would introduce additional complexity to the model. However, some researchers have experimented with using dropout in pooling layers, and some have found that it can improve the performance of the model.\n",
    "\n",
    "\n",
    "**Here are some guidelines for using dropout in CNNs:**\n",
    "\n",
    "\n",
    "**Convolutional layers**: Dropout can be used in convolutional layers to prevent overfitting. A good starting point is to use a dropout percentage of 0.2 or 0.3.\n",
    "\n",
    "\n",
    "**Pooling layers**: Dropout is not typically used in pooling layers.\n",
    "\n",
    "\n",
    "**Fully connected layers**: Dropout can be used in fully connected layers to prevent overfitting. A good starting point is to use a dropout percentage of 0.5 or 0.6.\n",
    "\n",
    "----------------------------------------------------\n",
    "\n",
    "**Good Practice for neural networks**\n",
    "\n",
    "\n",
    "**Smaller layers**: Use a lower dropout percentage, such as 0.1 or 0.2.\n",
    "\n",
    "\n",
    "**Larger layers**: Use a higher dropout percentage, such as 0.3 or 0.4.\n",
    "\n",
    "\n",
    "**More complex tasks**: Use a higher dropout percentage.\n",
    "\n",
    "\n",
    "**Less training data**: Use a higher dropout percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115e5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d915826d",
   "metadata": {},
   "source": [
    "### Which optimizer, loss function and metric to use in CNN:- \n",
    "\n",
    "* Use a binary crossentropy loss function, which is a good choice for binary classification tasks and categorical crossentrophy for multi-class classification tasks.\n",
    "\n",
    "\n",
    "* Use the Adam optimizer, which is a good choice for most deep learning tasks.\n",
    "\n",
    "\n",
    "* Use the accuracy metric, which is a good choice for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9e919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56db6d9d",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "\n",
    "* Data augmentation is a technique used to artificially increase the size of a dataset by applying various transformations to the existing images, such as rotations, flips, translations, zooming, etc. While performing data augmentation, the total number of images may remain the same, but the dataset's diversity and variability increase due to the transformed images.\n",
    "\n",
    "\n",
    "* However, it's important to note that data augmentation does not increase the unique images in the dataset. The augmented images are generated on-the-fly during training and do not create additional unique images. They are applied dynamically during each training epoch, allowing the model to see different variations of the original images and improve its generalization.\n",
    "\n",
    "\n",
    "* So, the number of unique images in your dataset remains 5000, but the model effectively sees a more diverse and varied dataset due to data augmentation, which can help improve the model's performance and robustness.\n",
    "\n",
    "### `IMPORTANT`\n",
    "\n",
    "\n",
    "* **Data augmentation is a technique that can be used to artificially increase the size of a training dataset by creating modified copies of images in the dataset. This can help to prevent overfitting, which is a problem that can occur when a model is trained on a small dataset.**\n",
    "\n",
    "\n",
    "* **Data augmentation Increases training data, Reduces overfitting and improves performance of model.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1dbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8668072c",
   "metadata": {},
   "source": [
    "### Perfect Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bef8f7",
   "metadata": {},
   "source": [
    "* **We can add multiple convolutional layer in 1 convolutional layers before pooling layer.**\n",
    "\n",
    "\n",
    "* **We can use batch normalization in dense layers also.**\n",
    "\n",
    "\n",
    "* **We can use data augmentation while dataset to prevent model from data augmentation.**\n",
    "\n",
    "\n",
    "* **If you want to add more data to pretrained model and do not rebuild their fully connected architecture, build model on same architecture of fully-connected layers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', input_shape=(256, 256, 3))) # for RGB Image\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), input_shape=(256, 256, 3)))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', input_shape=(256, 256, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d821672",
   "metadata": {},
   "source": [
    "**Add object of early stopping parameters here then continue with below code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data and validate on the validation data\n",
    "history = model.fit(train_ds, epochs=20, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd7860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a23e742",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Q) How to choose appropriate number of filters at every convolutional layer.\n",
    "\n",
    "\n",
    "A common practice is to start with a small number of filters in the first layer and gradually increase the number of filters in subsequent layers. For example, in a basic CNN architecture, the number of filters might be set as follows:\n",
    "\n",
    "**`First Convolutional Layer`**: Fewer filters, e.g., 32 or 64.\n",
    "\n",
    "\n",
    "**`Intermediate Convolutional Layers`**: Increasing number of filters, e.g., 128, 256, etc.\n",
    "\n",
    "\n",
    "**`Final Convolutional Layer`**: The number of filters can be set to the number of classes in the classification task (for classification problems).\n",
    "\n",
    "**Note**\n",
    "* The choice of the number of filters can also depend on the available computational resources and the size of the dataset. \n",
    "\n",
    "\n",
    "* Larger numbers of filters increase the loss of information and small size filter increases computational cost and the risk of overfitting if the dataset is small.\n",
    "\n",
    "\n",
    "* That's we use (3,3) filter becuase this size is not too big or small and in industries (3,3) filter size usually use. But we can also ue (1,1), (5,5), (7,7), (11,11) filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8088e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39e70bc6",
   "metadata": {},
   "source": [
    "### Q) How to specify correct input shape of images while creating dataset?\n",
    "\n",
    "\n",
    "The choice of the image size (image_size) when creating a dataset of images depends on various factors, including the characteristics of the dataset, the available computational resources, and the requirements of the specific deep learning model or task you are working on.\n",
    "\n",
    "\n",
    "As a starting point, image sizes such as (256, 256) or (224, 224) are commonly used in various computer vision tasks. These sizes are often chosen because they strike a balance between maintaining relevant details in the images and computational efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2421e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "552b9b63",
   "metadata": {},
   "source": [
    "### Q) What will happen if we incease the number of convolutional layers?\n",
    "\n",
    "\n",
    "Adding more convolutional layers can make the model more complex and difficult to train, but it can also improve the model's performance by extracting more features and reducing overfitting.\n",
    "\n",
    "\n",
    "**`Increased complexity`**: Adding more convolutional layers will increase the complexity of the CNN model. This can make the model more difficult to train, but it can also improve the model's performance.\n",
    "\n",
    "\n",
    "**`Improved feature extraction`**: More convolutional layers can extract more features from the input data. This can improve the model's ability to recognize patterns in the data and make more accurate predictions.\n",
    "\n",
    "\n",
    "**`Reduced overfitting`**: Adding more convolutional layers can help to reduce overfitting. This is because the model will be able to learn more features from the data, which will make it less likely to memorize the training data and generalize better to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20286b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b58c0e3d",
   "metadata": {},
   "source": [
    "### `'mini-batch'` parameter\n",
    "\n",
    "The size of the mini-batch is a hyperparameter that can be tuned to improve the performance of the model. A smaller mini-batch size can help to prevent overfitting, but it can also make the training process slower. A larger mini-batch size can speed up the training process, but it can also increase the risk of overfitting.\n",
    "\n",
    "The optimal mini-batch size depends on the specific problem and the dataset. However, a good starting point is a mini-batch size of 32 to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531faab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
