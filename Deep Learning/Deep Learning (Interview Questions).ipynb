{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674bf629",
   "metadata": {},
   "source": [
    "### Q) What is neurons in human body?\n",
    "\n",
    "Neurons are known as nerve cells. They are specialized cells which are connected to the other neurons, cells and muscles of the body and form a complex network throughout the body. The function of neurons are passing the internal and external information from one body part to another, specially brain and brain commands on action taken by the body parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5375e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9735401b",
   "metadata": {},
   "source": [
    "### Q) What is deep learning?\n",
    "\n",
    "Deep learning is a type of machine learning that uses artificial neural networks to learn from data. Neural networks are inspired by the human brain, and they are able to learn complex patterns from data. Deep learning has been used to achieve state-of-the-art results in many different areas, including image recognition, natural language processing, and speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c74e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21f8d1a7",
   "metadata": {},
   "source": [
    "### Q) What is neuron in neural networks?\n",
    "\n",
    "Neurons are the basic building blocks of neural networks. They are also known as nodes or units. Neurons take information from input data or from other neurons in the previous layer, perform calculations on that data, and generate an output. This output is then taken as input by other neurons in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f3784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59ad7338",
   "metadata": {},
   "source": [
    "### Q) What are the key components of a neural network?\n",
    "\n",
    "The key components of a neural network include the input layer, hidden layers, output layer, activation functions, weights, biases, and the optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c6d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22cab756",
   "metadata": {},
   "source": [
    "### Q) What are ANN (artificial neural networks)?\n",
    "\n",
    "Artificial neural networks are computational models inspired by the structure and function of the human brain. They consist of interconnected nodes (neurons) organized into layers, where each neuron processes and transfers information to other neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b62b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ddf78d8",
   "metadata": {},
   "source": [
    "### Q) What is the basic building block of a neural network?\n",
    "\n",
    "The basic building block is a neuron (or node). It takes inputs, applies weights to them, adds a bias term, performs an activation function, and produces an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824bee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "617836da",
   "metadata": {},
   "source": [
    "### Q) What is backpropagation?\n",
    "\n",
    "Backpropagation is the process by which a neural network adjusts its weights and biases during training to minimize the error. It uses the chain rule of calculus to calculate gradients and update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c29ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b221f23",
   "metadata": {},
   "source": [
    "### Q) What is the vanishing gradient problem?\n",
    "\n",
    "The vanishing gradient problem occurs in deep neural networks when the gradients / slopes become extremely small during backpropagation. This means that the updated weights are approximately equal to the old weights, and the optimizer has no effect on reducing the error / loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6377e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "108cd6dd",
   "metadata": {},
   "source": [
    "### Q) What is dropout in neural networks?\n",
    "\n",
    "Dropout is a regularization technique used to prevent overfitting in neural networks. During training, random neurons are dropped (set to zero) with a certain probability, reducing interdependence and improving generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7e365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d398cc5",
   "metadata": {},
   "source": [
    "### Q) What is the difference between CNN and RNN?\n",
    "\n",
    "\n",
    "* Convolutional Neural Networks (CNNs) are designed for image-related tasks and take advantage of spatial information.\n",
    "\n",
    "\n",
    "* Recurrent Neural Networks (RNNs) are suitable for sequence data, such as natural language, due to their ability to retain contextual information through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c854c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53bced25",
   "metadata": {},
   "source": [
    "### Q) What is transfer learning?\n",
    "\n",
    "* Transfer learning is a technique that can be used to improve the performance of a deep learning model. Transfer learning involves using a pre-trained model as a starting point for training a new model. The pre-trained model has already learned from previous features, so the new model can use those features to learn a new task.\n",
    "\n",
    "\n",
    "* In the context of CNNs, transfer learning means taking the data from a pre-trained model and customizing it to create a new model that is more effective than the pre-trained model. For example, if there is a pre-trained model that can classify images of dogs, we can customize it to create a new neural network that can predict the breed of the dog in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d5ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3215d04",
   "metadata": {},
   "source": [
    "### Q) Explain overfitting in deep learning.\n",
    "\n",
    "* Overfitting occurs when a neural network performs well on the training data but poorly on unseen data. It means the model has memorized the training data without learning general patterns.\n",
    "\n",
    "\n",
    "* The problem of overfitting arises when we icrease the number of neurons and hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7c323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfafa5f5",
   "metadata": {},
   "source": [
    "### Q) How can you prevent overfitting?\n",
    "\n",
    "Techniques to prevent overfitting include:-\n",
    "\n",
    "* Using more data, \n",
    "\n",
    "\n",
    "* Applying regularization (e.g., L1 or L2 regularization)\n",
    "\n",
    "\n",
    "* Dropout layers\n",
    "\n",
    "\n",
    "* Early stopping\n",
    "\n",
    "\n",
    "* Using data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bfe74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15e5518b",
   "metadata": {},
   "source": [
    "### Q) What is Loss Function in NEural Network?\n",
    "\n",
    "Loss Function in neural network is used to calculate the error of neural network (Difference between predicted value and actual value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2e9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e62fb3",
   "metadata": {},
   "source": [
    "### Q) Difference between Loss & Cost Function in DL?\n",
    "\n",
    "Loss function is used to calculate error of  single data points and cost function is used to calculate average error of all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49297cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59e7a801",
   "metadata": {},
   "source": [
    "### Q) What is data augmentation?\n",
    "\n",
    "Data augmentation is a data preprocessing technique commonly used in deep learning, especially in Convolutional Neural Networks (CNNs). It involves applying various transformations or modifications to the existing training data to create additional samples. The goal of data augmentation is to increase the diversity and size of the training dataset without collecting new data manually. \n",
    "\n",
    "\n",
    "The process of data augmentation typically involves applying transformations such as:\n",
    "\n",
    "\n",
    "* **Rotation**: Rotating the image by a certain angle to simulate variations in camera angles or orientations.\n",
    "\n",
    "\n",
    "* **Horizontal and Vertical Flipping**: Mirroring the image horizontally or vertically to create new samples.\n",
    "\n",
    "\n",
    "* **Translation**: Shifting the image in different directions to represent changes in the object's position.\n",
    "\n",
    "\n",
    "* **Scaling and Zooming**: Resizing the image to simulate different object sizes or zooming effects.\n",
    "\n",
    "\n",
    "* **Brightness and Contrast Adjustment**: Changing the brightness and contrast of the image.\n",
    "\n",
    "\n",
    "* **Noise Addition**: Introducing random noise to the image to mimic real-world variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14931c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ffae14d",
   "metadata": {},
   "source": [
    "### Q) What are some of the most common applications of deep learning?\n",
    "Some of the most common applications of deep learning include:\n",
    "\n",
    "* `Image recognition`: Deep learning has been used to develop algorithms that can recognize objects in images. This has been used for tasks such as facial recognition, object detection, and scene classification.\n",
    "\n",
    "\n",
    "* `Natural language processing`: Deep learning has been used to develop algorithms that can understand and process natural language. This has been used for tasks such as machine translation, text summarization, and question answering.\n",
    "\n",
    "\n",
    "* `Speech recognition`: Deep learning has been used to develop algorithms that can recognize speech. This has been used for tasks such as voice search, dictation, and customer service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d963ae0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ce2b8d",
   "metadata": {},
   "source": [
    "### Q) What is forword and backward propogation?\n",
    "\n",
    "* Forward propagation is the process of passing the input data through the neural network to calculate the output.\n",
    "\n",
    "\n",
    "* Backward propagation is the process of calculating the gradients of the loss function with respect to the weights of the neural network. The gradients are then used to update the weights of the neural network and optimize the performance of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed6352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ec757c5",
   "metadata": {},
   "source": [
    "### Q) What is epocha?\n",
    "\n",
    "An epoch refers to the number of times the entire training dataset passes through the neural network in forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b5228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4258245",
   "metadata": {},
   "source": [
    "### Q) What is derivatives?\n",
    "\n",
    "* Derivatives represents the rate of change in one quantity with respect to other quantity. It means, change of 1 unit in quantity, how it effects the other quantity. \n",
    "\n",
    "\n",
    "* Derivates are used in neural networks to update the weights of neural network during trainning process. This process is known as backward propogation.\n",
    "\n",
    "\n",
    "* The work of derivates is to calculate 1 unit chanhe in weight, how it effects the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51733c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9e00e0c",
   "metadata": {},
   "source": [
    "### Q) What are activation functions and their role in neural networks?\n",
    "\n",
    "* Activation functions are matehmatical functions which are applied to the output of neurons. It ensures that whether the neuron is activated or not based on the input it receives. It also builds non-linearity in the networks.\n",
    "\n",
    "\n",
    "* Activation Functions helps the neural network to learn the complex patterns of data more effectively by introducing non-linearity in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30c997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f8b257d",
   "metadata": {},
   "source": [
    "### Q) Which activation function and when to use?\n",
    "\n",
    "|Activation Function|Description|When to Use|\n",
    "|---------|----------------------|----------|\n",
    "|Sigmoid|Non-linear function with a sigmoid shape. Output values between 0 and 1.|Classification problems|\n",
    "|Tanh\tS|imilar to the sigmoid function, but has a range of [-1, 1].\t|Classification and regression problems|\n",
    "|ReLU\t|Non-linear function with a linear shape for positive inputs and a zero output for negative inputs. Computationally efficient.\t|Classification and regression problems|\n",
    "|Leaky ReLU\t|Variant of the ReLU function that has a small positive output for negative inputs. Less prone to the vanishing gradient problem than the ReLU function.\t|Classification and regression problems|\n",
    "|Softmax\t|Normalization function that outputs a vector of probabilities that sum to 1.|\tClassification problems|\n",
    "\n",
    "------------------\n",
    "\n",
    "**`Regression`**: ReLU/Leaky ReLU/P ReLU + Linear\n",
    "\n",
    "\n",
    "**`Binary-Classification`**: ReLU/Leaky ReLU/P ReLU + Sigmoid Activation Function\n",
    "    \n",
    "\n",
    "**`Multi-Class Classification`**: ReLU/Leaky ReLU/P ReLU + Softmax Activation Function\n",
    "\n",
    "---------------------\n",
    "\n",
    "1) **`Sigmoid Activation Function`**\n",
    "\n",
    "\n",
    "Equation: σ(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "\n",
    "Range: (0, 1)\n",
    "\n",
    "\n",
    "Derivative Range: (0, 0.25)\n",
    "\n",
    "\n",
    "**`Advantage`**: \n",
    "\n",
    "* The sigmoid function is a non-linear function that has a sigmoid shape.\n",
    "\n",
    "\n",
    "*  it can output values between 0 and 1, which can be interpreted as probabilities.\n",
    "\n",
    "\n",
    "* It is usually used with binary classififcation problem.\n",
    "\n",
    "\n",
    "**`Limitations`**:\n",
    "\n",
    "\n",
    "* Due to low range (0-0.25) the problem of vanishing gradient descent is created.\n",
    "\n",
    "\n",
    "* It doesn't has zero centric curve, it means after applying Sigmoid Activation Function, value of weights always comes in positive only not in negative.\n",
    "\n",
    "\n",
    "* It equation is complex, it means executing time is more.\n",
    "\n",
    "\n",
    "**`Note`**: To deal with problem of zero centric curve, scientist campe up with second activtion function Tanh.\n",
    "\n",
    "----------------\n",
    "\n",
    "2) **`Tanh (Hyperbolic Tangent) Activation Function:`**\n",
    "\n",
    "\n",
    "Equation: tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "\n",
    "Range: (-1, 1)\n",
    "\n",
    "\n",
    "Derivative Range: (0, 1)\n",
    "\n",
    "\n",
    "**`Advantage`**:\n",
    "\n",
    "\n",
    "* The tanh function is similar to the sigmoid function, but it has a range of [-1, 1].\n",
    " \n",
    "\n",
    "* It prevents from zero centric curve problem, it means value of weights comes in both postive and negative.\n",
    "\n",
    "\n",
    "* The range is more than sigmoid.\n",
    "\n",
    "\n",
    "* Used in Regression problems\n",
    "\n",
    "\n",
    "**`Limitations`**:\n",
    "\n",
    "\n",
    "* The equation is more complex than sigmoid and take more time in execution.\n",
    "\n",
    "-------------------\n",
    "\n",
    "3) **`ReLU (Rectified Linear Unit) Activation Function`**:\n",
    "\n",
    "\n",
    "Equation: f(x) = max(0, x)\n",
    "\n",
    "\n",
    "Range: [0, +∞)\n",
    "\n",
    "\n",
    "Derivative Range: {0 for x < 0, 1 for x > 0}\n",
    "\n",
    " \n",
    "**`Advantage`**:\n",
    "\n",
    "\n",
    "* This is much faster than Sigmoid and Tanh in execution due to it's simple formula.\n",
    "\n",
    "\n",
    "* The ReLU (Rectified Linear Unit) function is a non-linear function that has a linear shape.\n",
    "\n",
    "\n",
    "* It is a very popular activation function in deep learning, as it is computationally efficient and can help to prevent the vanishing gradient problem.\n",
    "\n",
    "\n",
    "* Usually used in both regression and classification in hidden layer.\n",
    "\n",
    "\n",
    "**`Limitations`**:\n",
    "\n",
    "\n",
    "* Derivative range is 0-1. So, in case if value of derivative is zero than the neuron is destory because in this case old weight = new weight. This is known as died ReLU problem.\n",
    "\n",
    "\n",
    "**`Note`**: To deal with problem of died Relu, scientist came up with Leaky ReLU.\n",
    "\n",
    "--------------------------\n",
    "\n",
    "\n",
    "4) **`Leaky ReLU`**: The Leaky ReLU function is a variant of the ReLU function that has a small positive output for negative inputs. This makes it less prone to the vanishing gradient problem than the ReLU function.\n",
    "\n",
    "Equation: f(x) = max(0.01x, x)\n",
    "\n",
    "\n",
    "Range: [0, +∞)\n",
    "\n",
    "\n",
    "Derivative range: [0.01, 1]\n",
    "\n",
    "\n",
    "**`Advantage`**: \n",
    "\n",
    "\n",
    "* The Leaky ReLU function is a variant of the ReLU activation function.\n",
    "\n",
    "\n",
    "* It solves the problem of died ReLU because it equation is max(0.01x, x), where x can not be 0.\n",
    "\n",
    "\n",
    "* Usually used in both regression and classification in hidden layer.\n",
    "\n",
    "\n",
    "**`Limitations`**:\n",
    "\n",
    "* It is less prone to the vanishing gradient problem than the ReLU function.\n",
    "\n",
    "---------------\n",
    "\n",
    "5) **`ELU (Exponential Linear Unit) Activation Function:`**: \n",
    "\n",
    "Equation: f(x) = α * (exp(x) - 1) for x < 0, x for x >= 0, where α is a small positive constant (e.g., 1.0)\n",
    "\n",
    "\n",
    "Range: (-∞, +∞)\n",
    "\n",
    "\n",
    "Derivative Range: {α * exp(x) for x < 0, 1 for x >= 0}\n",
    "\n",
    "\n",
    "**`Advantage`**: \n",
    "\n",
    "\n",
    "* We can customize the value of α (Alpha).\n",
    "\n",
    "\n",
    "**`Limitations`**:\n",
    "\n",
    "\n",
    "* We can not use it anywhere, means it is not a important activation function.\n",
    "\n",
    "-------------------------\n",
    "\n",
    "6) **`Softmax  Activation Function (for Multiclass Classification)`**\n",
    "\n",
    "Equation: σ(x_i) = exp(x_i) / Σ(exp(x_j)) for all i, where x_i is the input to the i-th class, and the sum is over all classes.\n",
    "\n",
    "\n",
    "Range: (0, 1)\n",
    "\n",
    "\n",
    "Derivative Range: (0, 1)\n",
    "\n",
    "\n",
    "**`Advantage`**: \n",
    "\n",
    "\n",
    "* In formula which label has highest value of x it will take that label as output.\n",
    "\n",
    "\n",
    "* The softmax function is a normalization function that is often used in  multi-class classification problems.\n",
    "\n",
    "\n",
    "* It is  commonly used in the output layer for multi-class classification problems.\n",
    "\n",
    "-----------\n",
    "\n",
    "\n",
    "7) **`Parametric ReLU (PReLU) Activation Function`**: \n",
    "\n",
    "\n",
    "Equation: f(x) = max(αx, x), where α is a learnable parameter during training\n",
    "\n",
    "\n",
    "Range: (-∞, +∞)\n",
    "\n",
    "\n",
    "Derivative Range: {α for x < 0, 1 for x > 0}\n",
    "\n",
    "\n",
    "**`Advantage`**:  \n",
    "\n",
    "* It is the combination of ReLU and Leaky ReLU that has a learnable parameter α (Alpha).\n",
    "\n",
    "\n",
    "* It is used in both in both Regression and classification problem.\n",
    "\n",
    "\n",
    "**`Limitations`**:\n",
    "\n",
    "* We use P ReLU where ReLU and Leaky ReLU fails.\n",
    "\n",
    "\n",
    "8) **`SWISH (Smoothed Exponential Linear Unit)`**:\n",
    "\n",
    "\n",
    "Equation: f(x) = x * sigmoid(x * 3)\n",
    "\n",
    "\n",
    "**`Advantage`**:  \n",
    "\n",
    "\n",
    "* SWISH are non-linear activation functions and advance version of sigmoid function.\n",
    "\n",
    "\n",
    "**`Limitations`**:\n",
    "\n",
    "\n",
    "* This is a relatively new activation function and not as commonly used as the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656d32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3d85697",
   "metadata": {},
   "source": [
    "### Q) What are weights in neural network?\n",
    "\n",
    "Weights are numerical values that connect one neuron to another. The weights are updated many times during the learning process. The goal of neural networks is to find the correct set of weights that will allow the neural network to perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04083608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b247bb8e",
   "metadata": {},
   "source": [
    "### Q) What is bias?\n",
    "\n",
    "\n",
    "\n",
    "Bias is a learnable parameter in neural networks that is used to adjust the weights of the network. Every neuron has its own bias, which is added to the weighted sum of the inputs to the neuron and apply activation function over it to generate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c91a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "026b2572",
   "metadata": {},
   "source": [
    "### Q) What is Tensor in neural networks?\n",
    "\n",
    "Tensor is a type data structure similar to array or a matrix. Tensors are used to store information and represents input data such as weights, bias, gradients and outputs of neuron.\n",
    "\n",
    "Tensors can have different ranks, which define their dimensions:\n",
    "\n",
    "A rank-0 tensor is a scalar (a single value).\n",
    "\n",
    "\n",
    "A rank-1 tensor is a vector (an array of values).\n",
    "\n",
    "\n",
    "A rank-2 tensor is a matrix (a 2-dimensional array).\n",
    "\n",
    "\n",
    "A rank-3 tensor is a 3-dimensional array, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cdea6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56e3e093",
   "metadata": {},
   "source": [
    "### Q) What is Dropout regularization?\n",
    "Dropout regularization is a technique used in neural networks to reduce overfitting. During training, a fraction of randomly selected neurons is temporarily \"dropped out\" or deactivated. This prevents the network from relying too heavily on specific neurons and encourages more robust and generalized learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc188dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
