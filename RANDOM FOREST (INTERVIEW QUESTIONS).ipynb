{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf107c72",
   "metadata": {},
   "source": [
    "### Parameters and thier use in Random Forest:-\n",
    "#### max_samples \n",
    "* Max_sample is used to set the number of samples for each random subset. \n",
    "\n",
    "\n",
    "* By default it is 'none' means all the samples will take in all random subsets but with rows replacement.\n",
    "\n",
    "\n",
    "* We can set this parameter in integer as well as in float(0-1).\n",
    "\n",
    "\n",
    "* Float means how much percentage of rows will take by each subset and int means no. of rows take by each subset.\n",
    "\n",
    "\n",
    "* max_samples = 0.9 or max_samples = 90,100,52, etc.\n",
    "\n",
    "#### max_features \n",
    "\n",
    "* By default max_features = 'auto'\n",
    "\n",
    "\n",
    "* Max_features is use to set a number of feature to create a random subset of data.\n",
    "\n",
    "\n",
    "* There are 3 options available to set this parameter which are 'auto', 'sqrt', 'log2'\n",
    "\n",
    "\n",
    "* max_features = 'auto' means it will take square root of total number of features and put features in random subset according that value. For eg: total number of columns are 10, square root of 10 is 3.16 so it will take 3 features randomly from trainning data to create a subset. But in decision tree algorithm this parameter takes all features when it set to auto.\n",
    "\n",
    "\n",
    "* max_features = 'sqrt' means it will work same as auto in random forest. \n",
    "\n",
    "\n",
    "* max_features = 'log' means it will multiple the total number of features to the value of log2 which is 0.3010 and what the values comes it will take that value to put features in random subset. Eg: total number of features are 10, 0.3010 * 10 = 3.010, So it will randomly take 3 features to create random subset.   \n",
    "\n",
    "\n",
    "* We can set this parameter in integer as well as in float(0-1).\n",
    "\n",
    "\n",
    "* Float means how much percentage of feature will take by each subset and int means no. of columns take by each subset.\n",
    "\n",
    "\n",
    "* max_features = 0.9 or max_features = 10,12,15,20, etc.\n",
    "\n",
    "#### bootstrap\n",
    "\n",
    "* Bootstrap parameter is used to define that create subset with row sampling with replacement or not.\n",
    "\n",
    "\n",
    "* By default bootstrap = True, means create subset with row sampling with replacement.\n",
    "\n",
    "\n",
    "* If bootstrap = False, means take all rows to create subset not take random rows.\n",
    "\n",
    "\n",
    "* Row sampling with replacement means some rows repeat again in same subset.\n",
    "\n",
    "\n",
    "* It will take approx 66.66% of total rows from original data to make a subset because some of the rows are repeating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0592e538",
   "metadata": {},
   "source": [
    "Q: What are the advantages of using Random Forest?\n",
    "* A: Random Forest has several advantages over other machine learning algorithms. It is highly accurate, can handle large datasets, is less prone to overfitting, and can handle missing values and categorical data.\n",
    "\n",
    "\n",
    "\n",
    "Q: What is the out-of-bag error in Random Forest?\n",
    "* A: The out-of-bag (OOB) error is the error rate of a Random Forest model on the training data that was not used to build each individual decision tree. This allows for a more accurate estimate of the generalization error of the model.\n",
    "\n",
    "Q: What is the feature importance in Random Forest?\n",
    "* A: Feature importance is a measure of how much each feature contributes to the accuracy of a Random Forest model. It is calculated by looking at the decrease in the impurity of the nodes when a particular feature is used to split the data. The higher the decrease in impurity, the more important the feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341d88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677a9428",
   "metadata": {},
   "source": [
    "Q1) What is Random Forest?\n",
    "* Random Forest is an ensemble learning algorithm that builds multiple decision trees on random subsets of data and combines the result of all decision trees to make a final prediction. \n",
    "\n",
    "\n",
    "* It is a supervised learning algorithm that can be used for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e166ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a6a704b",
   "metadata": {},
   "source": [
    "Q2) What is bootstrap aggregation in random forest?\n",
    "\n",
    "* Bootstrap aggregation, also known as bagging, is a technique used in machine learning to improve the stability and accuracy of a model.\n",
    "\n",
    "\n",
    "* Bootstrap aggregation technique works by dividing the dataset into multiple random subsets by doing row sampling with replacement and feature sampling.\n",
    "\n",
    "\n",
    "* Then make a same model on every single random subset and combine their output to predict a final output.\n",
    "\n",
    "\n",
    "* This technique is commonly used in random forest to train multiple decision trees on different subsets of the data and improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f7546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef18e039",
   "metadata": {},
   "source": [
    "Q3) How does Random Forest work?\n",
    "* Random Forest builds multiple decision trees on random subsets of the data and combines the output of all the decision trees to make a final prediction. The goal is to reduce overfitting and improve the accuracy of the model.\n",
    "\n",
    "\n",
    "* Random subsets of data created by doing row sampling with replacement and feature sampling.\n",
    "\n",
    "\n",
    "* In case of regression problem it will take the average or mean of all predictions and give that value as a output.\n",
    "\n",
    "\n",
    "* In case of classification it will take that category which is most frequent among all and give that category as a output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c460d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fff20215",
   "metadata": {},
   "source": [
    "Q4) What is row sampling with replacement and feature sampling?\n",
    "\n",
    "\n",
    "* Row sampling and feature sampling means a subset of a data contains random rows and random features.\n",
    "\n",
    "\n",
    "* A subset contains equivalent number of rows to the original number of rows. Eg:- In data there are 100 rows so a random subset will also have 100 rows.\n",
    "\n",
    "\n",
    "* Row sampling with replacement means some of the rows will repeat in the same subset. A subset is create on approx 66.66% of original rows and approx 33.33% of rows are repeated within a same subset.\n",
    "\n",
    "\n",
    "* With the help of bootstrap parameter we can change it. By default the bootstrap parameter is set to True when we set it to False, then it will take all rows in all subsets without replacement.\n",
    "\n",
    "\n",
    "* We can also set the number or percentage of rows take by each subset by using max_samples parameter. Eg: Out of 100 rows we want only 90 rows in a subset. So, max_samples = 90 or max_samples = 0.90. Float is used to take a certain percentage of rows and integer is used to give exact number of rows.\n",
    "\n",
    "\n",
    "* Number of features to put in a subset set by max_feature parameter. By default it is set to 'sqrt' it means it will take the square root of the total of features and then create subset by using that number of features. Eg: their are 10 feature and their square root is 3.16 so, it will create random subsets by using only 3 features each. \n",
    "\n",
    "\n",
    "* But we can change it using max_features parameter. Eg: we want 5 features in each subset, then max_features = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752fcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0482e9f",
   "metadata": {},
   "source": [
    "Q5)  What is the difference between Random Forest and decision trees?\n",
    "\n",
    "\n",
    "* Decision trees are a single tree-based model that predicts the outcome based on a set of rules. \n",
    "\n",
    "\n",
    "* Random Forest is an ensemble of multiple decision trees that work together to make a final prediction.\n",
    "\n",
    "\n",
    "* Random Forest is generally more accurate and less prone to overfitting than a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a986caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff7686be",
   "metadata": {},
   "source": [
    "Q6) How random Forest predict the outcome by multiple-decision trees?\n",
    "\n",
    "\n",
    "* In case of regresssion problem it will take the mean or average of all outputs and predict a final output.\n",
    "\n",
    "\n",
    "* In case of classification problem it will predict a outcome which is most frequent among all. Eg: out of 10 decision trees, 7 decision trees predict yes and 3 predict no. So, the final output will be yes because it is the most frequent result among all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc49013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10ad5eb5",
   "metadata": {},
   "source": [
    "Q7) How do you tune the parameters of a Random Forest model?\n",
    "\n",
    "* The main parameters that can be tuned in a Random Forest model are the number of trees(n_estimators), the maximum depth(max_depth) of the trees, and the number of features(max_features) used to build each decision tree on that number of features, the criterion used for splitting (criterion = 'gini' / 'entropy'), there are several more parameters available in random_forest.\n",
    "\n",
    "\n",
    "* The best values for these parameters can be found using techniques like grid search or random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c639b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6708b73",
   "metadata": {},
   "source": [
    "Q8) Q: Can Random Forest be used for feature selection?\n",
    "\n",
    "* Yes, Random Forest can be used for feature selection by ranking the importance of each feature and then, selecting the top features.\n",
    "\n",
    "\n",
    "* This can help to reduce the dimensionality of the data and improve the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861c346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd8829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a24a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1d5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c542fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2a65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45a6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa79e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b08f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7679051b",
   "metadata": {},
   "source": [
    "Q5) What are the advantages of using Random Forest over Decision Trees and other Machine Learning Algorithms?\n",
    "\n",
    "Random Forest has several advantages over Decision Trees and other Machine Learning Algorithms:-\n",
    "\n",
    "\n",
    "* It can be used for solve both problems, regression as well as classification.\n",
    "\n",
    "\n",
    "* It is highly accurate and can handle large datasets.\n",
    "\n",
    "\n",
    "* It is less prone to overfitting because it uses row sampling and feature sampling to create random subsets of training data and then create multiple decision of random subsets. \n",
    "\n",
    "\n",
    "* It can handle missing values by dropping the observation which contains missing values.\n",
    "\n",
    "\n",
    "* Random Forest provides a measure of feature importance, which can help to identify the most important features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219c8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834eab32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
