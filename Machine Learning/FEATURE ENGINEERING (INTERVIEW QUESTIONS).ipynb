{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1efe5694",
   "metadata": {},
   "source": [
    "### Q) What is feature engineering?\n",
    "\n",
    "* Featuring engineering is a process of making new features, if needed and make all features ready for model building.\n",
    "\n",
    "\n",
    "* Featuring engineering involves creating new features, if needed, feature encoding, feature transformation, feature scaling and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67798acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3f2ab00",
   "metadata": {},
   "source": [
    "### Q) Why is feature engineering important?\n",
    "\n",
    "\n",
    "* Feature engineering is important to make the feature ready for model building. If we directly put the feature to a machine learning model then it will not take categorical features, if we put all numerical features then it will not provides better result. That's why in feature enginnering we encoding the features, transform the features, scale the features and select festures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f026db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "effff7f4",
   "metadata": {},
   "source": [
    "### Q) What are some common techniques used in feature engineering?\n",
    "\n",
    "Common techniques in feature engineering include:\n",
    "    \n",
    "1) Feature encoding\n",
    "\n",
    "\n",
    "2) Feature Transformation\n",
    "\n",
    "\n",
    "3) Feature scaling\n",
    "\n",
    "\n",
    "4) Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a2070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "239c516a",
   "metadata": {},
   "source": [
    "### Q) Why Feature scaling is important?\n",
    "\n",
    "* Feature scaling is important because features have different ranges in a dataset. Some feature have high range while some feature has low range. So, Feature scaling help to change the range of features and convert into a specific range.\n",
    "\n",
    "\n",
    "* We we directly put features into a machine learning model without doing scaling, so our model will biased toward those variables which have high ranges.\n",
    "\n",
    "\n",
    "### OR\n",
    "\n",
    "* We have multiple variables in a dataset with multiple ranges. Some variables have low range of distribution, some variables have very high range of distribution. \n",
    "\n",
    "\n",
    "* If we put these variables directly to machine learning model, our model biased towards the higher range variables. This is because the model might give more importance to variables with larger values, assuming that they are more important.\n",
    "\n",
    "\n",
    "* That's why we scale the data before putting it into ML model. Feature scaling techniques transform all the variables to be in same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ab70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c20be14",
   "metadata": {},
   "source": [
    "### Q) What is robust scaler?\n",
    "\n",
    "* Robust scaler is used to scale and chane the range of data within a specific range. We usually use robust scaler when data set contains outliers and we don't want to remove outliers.\n",
    "\n",
    "\n",
    "* Robust scaler subtract each data point by the median of data and divide by interquartile range.\n",
    "\n",
    "\n",
    "* Formula of robust scaler is Robust_Scaled_Value = (X - median(X)) / (Q3(X) - Q1(X)).\n",
    "\n",
    "\n",
    "* The range of the robust scaler is not fixed it depends on the nature of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99aa35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac4393df",
   "metadata": {},
   "source": [
    "### Q) What is Standardization / Z-Score Standarization?\n",
    "\n",
    "* Standardization is a feature scaling technique that is use to transform data and change it's range between -3 to +3 if data doesn't contain outliers. \n",
    "\n",
    "\n",
    "* Standard scaler helps in to change the mean of the data to 0 or around 0, which is usefull for many ML algorithms such as PCA.\n",
    " \n",
    "\n",
    "* Standardization use standard scaler to transform data. Standard scaler calculates Z-Score for each data point.\n",
    "\n",
    "\n",
    "* If our data doesn't contain outliers then the range of Z-Score and standard scaler is always ranges between -3 to +3 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13330d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc975629",
   "metadata": {},
   "source": [
    "### Q) What is Normalization?\n",
    "\n",
    "* Normalization is a feature scaling technique that is used to transform data and change it's range between 0-1.\n",
    "\n",
    "\n",
    "* Normalization use min-max scaler. The formula of min max scaler is - x_scaled = x - min value / max value - min value\n",
    "\n",
    "where,\n",
    "\n",
    "x = original value\n",
    "\n",
    "\n",
    "min = minimum value of data\n",
    "\n",
    "\n",
    "max = maximum value of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830a904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db4ba0cf",
   "metadata": {},
   "source": [
    " ### Q) Can we apply same Scaling Method for all variables?\n",
    "\n",
    "* Yes, it is recommended to use same scaler because we want the range of all variables to be same. If the variable is already is the same range so we can not scale them. Eg: we have any binary variable so we can not scale them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06412e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "275b4bc1",
   "metadata": {},
   "source": [
    "### Q) Why skewness is bad for our data?\n",
    "\n",
    "* If the skewness is present in the data than mean is not the appropriate measure of central tendency to fill the null values, in that case we consider median.\n",
    "\n",
    "\n",
    "* When the skewness is present in the data and we directly put skewed data in machine learning model then it will baised toward the majority values, it means it learns more about the relationship and pattern of majority values but we don't want this we want that our machine learning model learns equally on all values. So, after model building if we predict something by giving it value of independent variables which are in minority at the time of trainning phase, so it will not provides better results.\n",
    "To avoid skewness we use data transformation techniques such as logarithm transformation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2df7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c14af524",
   "metadata": {},
   "source": [
    "### Q) How would you handle categorical variables during EDA? / How to deal with categorical variables in machine learning or How to convert them into Numerical?\n",
    "\n",
    "Our Machine Learning model do not understand the categorical data that's why converting categorical data into numerical is necessary because machine learning algorithm works on mathematical and statistical equations that why encoding of data is necesaary. There are several techniques by which we can convert our categorical data into numerical, such as:- \n",
    "\n",
    "1) Label Encoder :- Label Encoder convert each category of a variable into a number. Eg:- We have 5 categories in a variable, so label encoder labels categories into a sequence, such as 0,1,2,3,4.\n",
    "\n",
    "\n",
    "2) df.replace():- Using df.replace() function we can convert each category into any number by our choice.\n",
    "\n",
    "\n",
    "3) get_dummies / One Hot Encoding:- Get_dummies method create a new column for each category and the values in the column are 0 & 1. If the category is present for the particular observation then the value in column is 1, otherwise 0. In get_dummies method we remove original column and remove any one category's column to prevent it from multi-collinearty. \n",
    "\n",
    "\n",
    "4) Target-Mean Encoding:- It will convert a each category of a variable according to their mean in the target variable. For example:- We have a catergory, so we check how many times this category repeats in a variable and check their target variable value, then add all the values of target variable for the specific category and take it's mean. This category will replace by it's mean in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11c952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa6225ef",
   "metadata": {},
   "source": [
    "### Q) Why we drop one variable in get_dummies?\n",
    "\n",
    "This is done to avoid multicollinearity in certain statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e621a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d0ce06b",
   "metadata": {},
   "source": [
    "### Q) When to use get dummies methods, label encoder and target mean encoding?\n",
    "\n",
    "Using a specific encoding technique will depends on the nature of data.\n",
    "\n",
    "* When we have nomial categorical variable, it is recommended to use one hot encoding or get_dummies method.\n",
    "\n",
    "\n",
    "* When we have ordinal categorical variable, it is recommended to use Label encoder or df.replace() method.\n",
    "\n",
    "\n",
    "* When we have multiple unique categories in the variable, such as location. It is recommended to use target mean encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80116bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c63be8f8",
   "metadata": {},
   "source": [
    "### Q) What is disadvantage of get_dummies method or One hot encoding?\n",
    "\n",
    "\n",
    "* By using get_dummies or One Hot Encoding method the number of variables will increase. Eg: we have 4 categories in a column so it will make 4 column, 1 column for each category, and we genrally remove any 1 column so, we have total 3 columns. Due to more number of columns the chances of overfitting will increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941c5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12730cda",
   "metadata": {},
   "source": [
    "### Q) Can we apply same Encoding to encode all variable?\n",
    "No, it is not recommended to use same encosing technique for all variables. We can use multiple encoding techniques in a dateset it depends on the nature of variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c94908c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7918d81",
   "metadata": {},
   "source": [
    "### Q) Can you provide an example of feature engineering?\n",
    "\n",
    "* We convert categorical data into numerical, transform the data to reduce skewness, scale the data, select important features for model building these all are examples of model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932fb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f215d2cb",
   "metadata": {},
   "source": [
    "### Q) What is the curse of dimensionality?\n",
    "\n",
    "* Curse of dimensionality is amconcept which states that there is a optimal number of features, if we put more or less features than that limit so, our machine learning model will not provides best results. It will provide best result only when we put optimial number of features in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc774c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebd8e704",
   "metadata": {},
   "source": [
    "### Q) How do you handle time-related data in feature engineering?\n",
    "\n",
    "* I will create a seprate column for hour, day, date, month and year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6c3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4850363b",
   "metadata": {},
   "source": [
    "### Q) What is feature selection, and why is it important in feature engineering?\n",
    "\n",
    "\n",
    "* Feature selection is a technique by which we can select a optimal number of features and select only those features which are important for model building.\n",
    "\n",
    "\n",
    "* If we directly put all the features into a model then it will be overfitted, so to reduce overfitting and complexity of model we use feature selection techniques, such as VIF, RFE, correlation analysis, Random Forest feature importance which uses information gain, PCA, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11caf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2604e63b",
   "metadata": {},
   "source": [
    "### Q) What are different types of Feature selection techniques?\n",
    "\n",
    "There are many feature selection techniques such as:\n",
    "\n",
    "1) Pearson's Correlation: In pearson's correlation we check each of correlation and according to that we drop and select features.\n",
    "\n",
    "\n",
    "2) RFE (Recursive Feature Elimination): In Recursive feature elimation it will provides a weightage of each variable and according to that we select a subset of features.\n",
    "\n",
    "\n",
    "3) Information gain: We use information gain using random forest or decision tree, it will provide feature importance for each feature and according to that we select only that feature which are high information gain.\n",
    "\n",
    "\n",
    "4) VIF: VIF stands for Variance inflaction Factor. It is a technique to find multi-colinearity among features, so we drop feature one by one which having high multi-collinearity. \n",
    "\n",
    "\n",
    "5) PCA: PCA is a dimensionality reduction technique by which we reduce number of features by considering only that principal components which carries high variance of data.\n",
    "\n",
    "\n",
    "Among these techniques there are lot of feature selection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075e27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eedba30a",
   "metadata": {},
   "source": [
    "### Q) How do you handle skewed or non-normal distributions in feature engineering?\n",
    "\n",
    "* Skewness means when the distribution of data is asymmetric around the mean. To remove skewness of data we use various transformation techniques such as logarithm transformation, box-cox transformation, square atransformation, square root transformation, resipocal transformation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a3f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9e4f22b",
   "metadata": {},
   "source": [
    "### Q) Can you explain the concept of feature importance in feature engineering?\n",
    "\n",
    "* Feature importance indicates to ranking of features according to their importance in ML model. Feature with high importance indicates that these feature are impact the most on target variable and on performance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f139d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c00f29a3",
   "metadata": {},
   "source": [
    "### Q) What are interaction features, and how do you create them?\n",
    "\n",
    "* Interaction features are those features which we make with the combination of 2 or more features. EG: We have housing data in which area of bathroom, living room, hall, etc. gives in sq. ft. so we add all these features to make a new feature which is total area of house in sq. ft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6441f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b07a56e",
   "metadata": {},
   "source": [
    "### Q) What is RFE?\n",
    "\n",
    "RFE (Recursive Feature Elimination) is a feature selection technique that works by recursively eliminating features from a given dataset. Here's how it works:\n",
    "\n",
    "1) Initially, a machine learning model is trained on the entire set of features.\n",
    "\n",
    "\n",
    "2) The importance or relevance of each feature is evaluated using a certain criterion (e.g., coefficient values in linear regression or feature importance in tree-based models).\n",
    "\n",
    "\n",
    "3) The least important feature(s) are then eliminated from the dataset.\n",
    "\n",
    "\n",
    "4) Steps 2 and 3 are repeated iteratively until a predetermined number of features is reached or until a stopping criterion is met (e.g., a specified number of features to select).\n",
    "\n",
    "\n",
    "5) Finally, the selected subset of features is used to train the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f41032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
