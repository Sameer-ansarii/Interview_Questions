{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5230ae1",
   "metadata": {},
   "source": [
    "### Q) What is Exploratory Data Analysis (EDA)?\n",
    "\n",
    "* Exploratory Data Analysis is the process of analizing and visualizing the data to gain insights from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fb4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4af1a7c",
   "metadata": {},
   "source": [
    "### Q) Why is EDA important in data analysis?\n",
    "\n",
    "* EDA is important in data analysis because it helps in understand the data, find patterns and relationships in data, identifying outliers in data and after analizing, gain insights from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1d5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75d0024d",
   "metadata": {},
   "source": [
    "### Q) What are the different steps involved in EDA?\n",
    "\n",
    "\n",
    "* There are various steps we perform in EDA like understand the data through statistical summary, data cleaning, handling outliers, impute missing values, understand the data through data visualization, check the distribution of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc6249a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "600eaa56",
   "metadata": {},
   "source": [
    "### Q) Explain the concept of missing value imputation in EDA /  How to fill null values?\n",
    "\n",
    "There are several ways to impute null values.\n",
    "\n",
    "* We can impute null values by measures of central tendency (Mean, Median and Mode). We first check for the outliers if outliers are present in the data then mean is not appropriate to fill null values.\n",
    "\n",
    "\n",
    "* We can use backward filling and forwarding filling method to fill null values, which means impute null values with the next previous observation's value or with the next observation's value.\n",
    "\n",
    "\n",
    "* We can make a regression model to predict the values using regression algorithms, such as linear regression, random forest regressor, etc.\n",
    "\n",
    "\n",
    "* We can use domian knowledge to fill null values.\n",
    "\n",
    "\n",
    "* We can imput null values with the client's help.\n",
    "\n",
    "\n",
    "* We can fill null values by using 99%tile value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f5a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb6c08a8",
   "metadata": {},
   "source": [
    "### Q) What are outliers?\n",
    "\n",
    "Outliers are extremly high or extremly low values which do not follow the pattern of data, which result in asymmetric data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7d45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a902b39",
   "metadata": {},
   "source": [
    "### Q) How do you handle outliers during EDA?\n",
    "\n",
    "* Treating the outliers is depends on the problem statement. If each individual observation is important then we can not remove it, whether it is a outlier. So, in that case we use data transformation and data scaling techniques to deal with outliers such as robust scaler and box-cox transformation.\n",
    "\n",
    "\n",
    "* If the outliers are necessary to remove then we use Z-Score Method or IQR Method to remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd26d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dfdfc39",
   "metadata": {},
   "source": [
    "### Q) What is frequency?\n",
    "Frequency represents how many times a particular category repeats in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c846b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5544c8e5",
   "metadata": {},
   "source": [
    "### Q) What is relative frequency?\n",
    "\n",
    "Relative Frequency is the percentage of each category repeats in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc58e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18388b1a",
   "metadata": {},
   "source": [
    "### Q) What are the common graphical techniques used in EDA? / What are the types of graphs used in EDA?\n",
    "\n",
    "\n",
    "* We use various types of graphs in EDA such as histogram to check the distibution of data within a specific range, barplot, pie plot, scatterplot, heatmaps, etc. By using these graphs we identify the pattern and relationship in data and gain insights from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569e12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8459403",
   "metadata": {},
   "source": [
    "### Q) How do you assess and handle data skewness during EDA?\n",
    "\n",
    "* Skewness means asymmetric data distribution. We reduce the skewness of data by appling various data transformation techniques such as logarithm transformation, box-cox transformation, etc.\n",
    "\n",
    "\n",
    "* We also reduce skewness of data by scaling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a13dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a4f936",
   "metadata": {},
   "source": [
    "### Q) What are the implications / impact of skewness for data analysis?\n",
    "\n",
    "* If the skewness is present in the data than mean is not the appropriate measure of central tendency to fill the null values, in that case we consider median.\n",
    "\n",
    "\n",
    "* When the skewness is present in the data and we directly put skewed data in machine learning model then it will baised toward the majority values, it means it learns more about the relationship and pattern of majority values but we don't want this we want that our machine learning model learns equally on all values. So, after model building if we predict something by giving it value of independent variables which are in minority at the time of trainning phase, so it will not provides better results.\n",
    "\n",
    "\n",
    "* To avoid skewness we use data transformation techniques such as logarithm transformation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22785c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "631bea41",
   "metadata": {},
   "source": [
    "### Q) What is kurtosis and their use?\n",
    "\n",
    "Kurtosis refers to the degree of peakedness of a distribution curve. It indicates how much data is concentrated around the mean of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c79f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb2e38e1",
   "metadata": {},
   "source": [
    "### What is the purpose of correlation analysis in EDA?\n",
    "\n",
    "* With the help of correlation analysis we understand the strength of directional relationship between variables. It is used in feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89458f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39ba83e4",
   "metadata": {},
   "source": [
    "### Q) How do you deal with multicollinearity during EDA?\n",
    "\n",
    "Multicollinearity happens when two or more independent variables are highly correlated. To reduce and avoid multicollinearity we use various techniques such as:-\n",
    "    \n",
    "    \n",
    "* Correlation matrix, we drop highly correlated variables using corrlation matrix.\n",
    "\n",
    "\n",
    "* We use VIF (Variance inflation factor) which is used to detect multicollinearity in data and drop those variables one by one which have high VIF value.\n",
    "\n",
    "\n",
    "* We also use PCA to reduce multicolinearity in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f149c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c628688b",
   "metadata": {},
   "source": [
    "### Q) Can you explain the concept of feature engineering in EDA?\n",
    "\n",
    "\n",
    "Featuring engineering is a process of making features ready for model building. \n",
    "\n",
    "* Featuring engineering involves creating new features, if needed, feature encoding, feature transformation, feature scaling and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a0b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da5a5bcb",
   "metadata": {},
   "source": [
    "### Q) What are some techniques for dimensionality reduction during EDA?\n",
    "\n",
    "* There are several techniques used for dimensionality reduction such as PCA, T-distributed Stochastic Neighbourhood Embedding (tSNE), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e4836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c557e778",
   "metadata": {},
   "source": [
    "### Q) How would you handle categorical variables during EDA? / How to deal with categorical variables in machine learning or How to convert them into Numerical?\n",
    "\n",
    "Our Machine Learning model do not understand the categorical data that's why converting categorical data into numerical is necessary because machine learning algorithm works on mathematical and statistical equations that why encoding of data is necesaary. There are several techniques by which we can convert our categorical data into numerical, such as:- \n",
    "\n",
    "1) Label Encoder :- Label Encoder convert each category of a variable into a number. Eg:- We have 5 categories in a variable, so label encoder labels categories into a sequence, such as 0,1,2,3,4.\n",
    "\n",
    "\n",
    "2) df.replace():- Using df.replace() function we can convert each category into any number by our choice.\n",
    "\n",
    "\n",
    "3) get_dummies / One Hot Encoding:- Get_dummies method create a new column for each category and the values in the column are 0 & 1. If the category is present for the particular observation then the value in column is 1, otherwise 0. In get_dummies method we remove original column and remove any one category's column to prevent it from multi-collinearty. \n",
    "\n",
    "\n",
    "4) Target-Mean Encoding:- It will convert a each category of a variable according to their mean in the target variable. For example:- We have a catergory, so we check how many times this category repeats in a variable and check their target variable value, then add all the values of target variable for the specific category and take it's mean. This category will replace by it's mean in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e8dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94beabd6",
   "metadata": {},
   "source": [
    "### Q) When to use get dummies methods, label encoder and target mean encoding?\n",
    "\n",
    "Using a specific encoding technique will depends on the nature of data.\n",
    "\n",
    "* When we have nomial categorical variable, it is recommended to use one hot encoding or get_dummies method.\n",
    "\n",
    "\n",
    "* When we have ordinal categorical variable, it is recommended to use Label encoder or df.replace() method.\n",
    "\n",
    "\n",
    "* When we have multiple unique categories in the variable, such as location. It is recommended to use target mean encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2aa959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "386e7c06",
   "metadata": {},
   "source": [
    "### Q) What is disadvantage of get_dummies method or One hot encoding?\n",
    "\n",
    "\n",
    "* By using get_dummies or One Hot Encoding method the number of variables will increase. Eg: we have 4 categories in a column so it will make 4 column, 1 column for each category, and we genrally remove any 1 column so, we have total 3 columns. Due to more number of columns the chances of overfitting will increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042f11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f2e6006",
   "metadata": {},
   "source": [
    "### Q) Can we apply same Encoding to encode all variable?\n",
    "No, it is not recommended to use same encosing technique for all variables. We can use multiple encoding techniques in a dateset it depends on the nature of variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe3ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22260a77",
   "metadata": {},
   "source": [
    " ### Q) Can we apply same Scaling Method for all variables?\n",
    "\n",
    "* Yes, it is recommended to use same scaler because we want the range of all variables to be same. If the variable is already is the same range so we can not scale them. Eg: we have any binary variable so we can not scale them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1ad63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0318df33",
   "metadata": {},
   "source": [
    "### Q) Synomims of independent variable and dependent variable?\n",
    "\n",
    "* Features VS Labels\n",
    "\n",
    "\n",
    "* Explantory VS Response Variable\n",
    "\n",
    "\n",
    "* Predictors VS Target\n",
    "\n",
    "\n",
    "* X VS Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48115565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c5b6971",
   "metadata": {},
   "source": [
    "### Q) If you have bad data means there are unwanted characters in your column, such as '>', '?', '=', etc. These character represents null values. What will you do to find all null values in the data?\n",
    "\n",
    "\n",
    "* We can replace the unwanted characters with np.nan and then use the df.isnull() function to identify the null values in each column. \n",
    "\n",
    "\n",
    "* np.nan function replace unwanted characters with 'nan' means not a number, which indicates a null value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bb625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42c5e6d9",
   "metadata": {},
   "source": [
    "### Q) What is univariate, bivariate, multivariate analysis?\n",
    "\n",
    "\n",
    "1) Univariate Analysis: Explore each variable seprately.\n",
    "    \n",
    "    \n",
    "2) Bivariate Analysis: Explore 2 variables together.\n",
    "    \n",
    "    \n",
    "3) Multivariate Analysis: Explore more than 2 variables together.\n",
    "\n",
    "\n",
    "* These analysis techniques are used to analize the data and gain insights from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cf9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d280b0c0",
   "metadata": {},
   "source": [
    "### Q) What is the role of EDA in feature selection?\n",
    "\n",
    "\n",
    "* EDA plays an important role in feature selection because in EDA we explore the features and understand that which feature is important for our machine learning model. We check correlation of each variable with target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd026d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a32ddd97",
   "metadata": {},
   "source": [
    "### Q) How to remove unwanted characters from column where values of column looks like 50,00,000."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7018d3a0",
   "metadata": {},
   "source": [
    "# Remove unwanted characters\n",
    "df['Column'] = df['Column'].str.replace(',', '')\n",
    "\n",
    "# Convert column to numeric data type\n",
    "df['Column'] = pd.to_numeric(df['Column'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adf88c",
   "metadata": {},
   "source": [
    "1) The str.replace() function is used to remove the commas (,) and unwanted characters from the 'Column'. This is done by replacing the commas with an empty string '' here.\n",
    "\n",
    "\n",
    "2) Next, the pd.to_numeric() function is used to convert the column to a numeric data type.\n",
    "\n",
    "\n",
    "3) The errors='coerce' parameter handles any values that cannot be converted to numeric, such as non-numeric value and characters or missing values, by replacing them with NaN (Not a Number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43981665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e4a21d",
   "metadata": {},
   "source": [
    "### Q) What is feature extraction, and when is it useful in feature engineering?\n",
    "\n",
    "\n",
    "* Feature extraction is a technique used for creating new features out of existing features. Feature extraction is useful for reduce multicollinearity, and it is also useful in reduce the dimensions of data. Usually we use PCA for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a400ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "853d2096",
   "metadata": {},
   "source": [
    "### Q) How do you handle categorical variables with high cardinality (variables with multiple categories)?\n",
    "\n",
    "* In case of categorical variables with high cardinality we use target mean encoding or frequency encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78377e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
