{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1791b15",
   "metadata": {},
   "source": [
    "# Emojis\n",
    "\n",
    "* There are 2-3 ways to deal with emojis whether to remove emojis or convert it into text that will on nature of task.\n",
    "\n",
    "\n",
    "* In Sentiment analysis we removed emojis in public platforms more people will post their thoughts, reviews, comment and comments or reviews will be this 'This is a very bad product (angry face emoji).' Here people can write text and then add a emoji at last of text. So, we don't need emojis here so we remove it.\n",
    "\n",
    "\n",
    "* In analyzing chat we keep emojis and convert emojis into text becuase emojis represents your mood in chat and emojis are important for analyzing chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62550f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c059a5b7",
   "metadata": {},
   "source": [
    "# Punctuation Marks\n",
    "\n",
    "* In resume-related tasks, we cannot remove punctuation marks because some words in a resume have meaning with punctuation. For example, the words \"B.Com\" have a different meaning than \"B Com\" if the punctuation mark is removed. Therefore, we need to keep the punctuation marks in the resume text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87940750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd3fa19d",
   "metadata": {},
   "source": [
    "# Stemming and lemmatization\n",
    "\n",
    "* We use lemmatization when we are making chatbots, because we need that words which has a english dictionary meaning.\n",
    "\n",
    "\n",
    "* These both are interlinked words, work of them is to convert words into its root form / base form. E.g.: go, going, gone will convert into go. This will help to perform calculations faster because number of columns will decrease by doing this. There is separate column for separate word. More vocabulary equivalent to a greater number of columns. We will see this feature engineering. That’s why we use stemming and lemmatization to reduce number of columns and speed up the training process.\n",
    "\n",
    "\n",
    "* After stemming, the meaning of the sentence may change. This is because a verb can become a noun after stemming and there may be a chance that the root word does not have an English dictionary meaning.\n",
    "\n",
    "\n",
    "* After lemmatization, the meaning of the sentence not change. This is because a verb can remian verb and noun can remian noun and the root word have an English dictionary meaning.\n",
    "\n",
    "\n",
    "* Lemmatization understand part of speech and working according it. We have to given that in which part of speech we want to convert text such as verb, noun, pronoun, adverb, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4562b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5302fa5",
   "metadata": {},
   "source": [
    "# Removal of Digits\n",
    "\n",
    "Removal of digits depends on tasks. Like you build model for sentiment analysis of a product where review is like ‘My delivery was supposed to reach here by 29th of May but it reached at 6th of June. So, I am very disappointing of service because delivery was late.’ Here we don’t need digits so we will remove them because ‘delivery was late’ mentioned in review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28276a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbaa406b",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "\n",
    "* It includes noun, pronoun, verb, adverb etc.\n",
    "\n",
    "\n",
    "* We see that 1 word has 2 meaning. E.g.:  \n",
    "\n",
    "\n",
    "'I would like to work in google.' Here: - Google is noun\n",
    "\n",
    "'I like to google things.' Here: - Google is verb.\n",
    "\n",
    "\n",
    "In both sentences google is used but their meaning is not same in both, means the part of speech is not same, means noun, pronoun, verb, adverb, etc. all these are called ‘Part of Speech’.\n",
    "\n",
    "\n",
    "* If we not perform part of speech, model will recognise all same words as same part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae955e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03f73c01",
   "metadata": {},
   "source": [
    "# Word Cloud\n",
    "\n",
    "* In sentiment analysis projects, we create word clouds for positive and negative words. We create separate word clouds for positive and negative words. In sentiment analysis, we have a training dataset. First, we create a word cloud of positive reviews and then a word cloud of negative reviews. We check which words are highly repeated in both positive and negative reviews. In positive reviews, which positive words are repeated the most? In negative reviews, which negative words are used the most? By doing this, we get an idea of the most commonly used positive and negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693c7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed979c24",
   "metadata": {},
   "source": [
    "# Spelling Correction\n",
    "\n",
    "* In spell correction it will either do spelling correction or auto correct. Example: It will convert 'Idia' into 'India' or 'Idea'. It will depends on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005649e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74887477",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "* It is used while making search engines and recommendation syasem is also used in search enginess. TF-IDF is used in search engines to rank websites based on how relevant they are to a user's search query. The more times a keyword appears in a website's content, the higher the website will rank in the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dee364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "470d75f6",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "* Perfrom Frequency Distribution / Removing most  or rare occuring irrelevant word after removing stop words.\n",
    "\n",
    "\n",
    "* In case of sentiment analysis and emotion detection projects, convert emoticons and emojis into text becuase they contains some valuable information about the review or comment.\n",
    "\n",
    "\n",
    "* If you are working on a language other than English, it is often easiest to convert the text into English first. This is because there is more information available about stopwords and pre-processing steps in English than in other languages. By converting the text into English, you can save yourself the time and effort of learning about the specific stopwords and pre-processing steps for the other language.\n",
    "\n",
    "\n",
    "* In natural language processing (NLP), we try out and evaluate different feature encoding techniques, build models on them, and choose the technique that gives the best results.\n",
    "\n",
    "\n",
    "* Analyse most occurring words that all are related to data. Check for suspicious words. If you find any word which is not helpful in solving task them remove that word. E.g.: In above vocabulary pune, mumbai words are there, so if we want then we can remove it becuase these words not help the model to classify resume.\n",
    "\n",
    "\n",
    "* For NLP tasks Multinomial Naive Bayes is used and for classification tasks in ML Gaussian Naive Bayes is used.\n",
    "\n",
    "\n",
    "* When evaluating a model, we can check the classification report for each category to see which categories have low recall or precision. If a category has a recall or precision of less than 0.50, we can work on that category to improve its recall and precision. This can be done by giving the model more data for that category. Increasing the amount of data for a category will help the model learn more about that category and make better predictions for it.\n",
    "\n",
    "\n",
    "* By looking at the classification report, you can see which categories the model is having trouble with. You can then remove those categories from the model and rebuild it. This will likely improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d89ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
