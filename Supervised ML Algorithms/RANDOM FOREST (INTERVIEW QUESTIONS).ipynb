{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf107c72",
   "metadata": {},
   "source": [
    "### Parameters and thier use in Random Forest:-\n",
    "#### max_samples \n",
    "* Max_sample is used to set the number of samples for each random subset. \n",
    "\n",
    "\n",
    "* By default it is 'none' means all the samples will take in all random subsets but with rows replacement.\n",
    "\n",
    "\n",
    "* We can set this parameter in integer as well as in float(0-1).\n",
    "\n",
    "\n",
    "* Float means how much percentage of rows will take by each subset and int means no. of rows take by each subset.\n",
    "\n",
    "\n",
    "* max_samples = 0.9 or max_samples = 90,100,52, etc.\n",
    "\n",
    "#### max_features \n",
    "\n",
    "* By default max_features = 'auto'\n",
    "\n",
    "\n",
    "* Max_features is use to set a number of feature to create a random subset of data.\n",
    "\n",
    "\n",
    "* There are 3 options available to set this parameter which are 'auto', 'sqrt', 'log2'\n",
    "\n",
    "\n",
    "* max_features = 'auto' means it will take square root of total number of features and put features in random subset according that value. For eg: total number of columns are 10, square root of 10 is 3.16 so it will take 3 features randomly from trainning data to create a subset. But in decision tree algorithm this parameter takes all features when it set to auto.\n",
    "\n",
    "\n",
    "* max_features = 'sqrt' means it will work same as auto in random forest. \n",
    "\n",
    "\n",
    "* max_features = 'log' means it will multiple the total number of features to the value of log2 which is 0.3010 and what the values comes it will take that value to put features in random subset. Eg: total number of features are 10, 0.3010 * 10 = 3.010, So it will randomly take 3 features to create random subset.   \n",
    "\n",
    "\n",
    "* We can set this parameter in integer as well as in float(0-1).\n",
    "\n",
    "\n",
    "* Float means how much percentage of feature will take by each subset and int means no. of columns take by each subset.\n",
    "\n",
    "\n",
    "* max_features = 0.9 or max_features = 10,12,15,20, etc.\n",
    "\n",
    "#### bootstrap\n",
    "\n",
    "* Bootstrap parameter is used to define that create subset with row sampling with replacement or not.\n",
    "\n",
    "\n",
    "* By default bootstrap = True, means create subset with row sampling with replacement.\n",
    "\n",
    "\n",
    "* If bootstrap = False, means take all rows to create subset not take random rows.\n",
    "\n",
    "\n",
    "* Row sampling with replacement means some rows repeat again in same subset.\n",
    "\n",
    "\n",
    "* It will take approx 66.66% of total rows from original data to make a subset because some of the rows are repeating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341d88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677a9428",
   "metadata": {},
   "source": [
    "Q1) What is Random Forest?\n",
    "* Random Forest is an ensemble learning algorithm that builds multiple decision trees on random subsets of data and combines the result of all decision trees to make a final prediction. \n",
    "\n",
    "\n",
    "* It is a supervised learning algorithm that can be used for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e166ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a6a704b",
   "metadata": {},
   "source": [
    "Q2) What is bootstrap aggregation in random forest?\n",
    "\n",
    "* Bootstrap aggregation, also known as bagging, is a technique used in machine learning to improve the stability and accuracy of a model.\n",
    "\n",
    "\n",
    "* Bootstrap aggregation technique works by dividing the dataset into multiple random subsets by doing row sampling with replacement and feature sampling.\n",
    "\n",
    "\n",
    "* Then make a same model on every single random subset and combine their output to predict a final output.\n",
    "\n",
    "\n",
    "* This technique is commonly used in random forest to train multiple decision trees on different subsets of the data and improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f7546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef18e039",
   "metadata": {},
   "source": [
    "Q3) How does Random Forest work?\n",
    "* Random Forest builds multiple decision trees on random subsets of the data and combines the output of all the decision trees to make a final prediction. The goal is to reduce overfitting and improve the accuracy of the model.\n",
    "\n",
    "\n",
    "* Random subsets of data created by doing row sampling with replacement and feature sampling.\n",
    "\n",
    "\n",
    "* In case of regression problem it will take the average or mean of all predictions and give that value as a output.\n",
    "\n",
    "\n",
    "* In case of classification it will take that category which is most frequent among all and give that category as a output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c460d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fff20215",
   "metadata": {},
   "source": [
    "Q4) What is row sampling with replacement and feature sampling?\n",
    "\n",
    "\n",
    "* Row sampling and feature sampling means a subset of a data contains random rows and random features.\n",
    "\n",
    "\n",
    "* A subset contains equivalent number of rows to the original number of rows. Eg:- In data there are 100 rows so a random subset will also have 100 rows.\n",
    "\n",
    "\n",
    "* Row sampling with replacement means some of the rows will repeat in the same subset. A subset is create on approx 66.66% of original rows and approx 33.33% of rows are repeated within a same subset.\n",
    "\n",
    "\n",
    "* With the help of bootstrap parameter we can change it. By default the bootstrap parameter is set to True when we set it to False, then it will take all rows in all subsets without replacement.\n",
    "\n",
    "\n",
    "* We can also set the number or percentage of rows take by each subset by using max_samples parameter. Eg: Out of 100 rows we want only 90 rows in a subset. So, max_samples = 90 or max_samples = 0.90. Float is used to take a certain percentage of rows and integer is used to give exact number of rows.\n",
    "\n",
    "\n",
    "* Number of features to put in a subset set by max_feature parameter. By default it is set to 'sqrt' it means it will take the square root of the total of features and then create subset by using that number of features. Eg: their are 10 feature and their square root is 3.16 so, it will create random subsets by using only 3 features each. \n",
    "\n",
    "\n",
    "* But we can change it using max_features parameter. Eg: we want 5 features in each subset, then max_features = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752fcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0482e9f",
   "metadata": {},
   "source": [
    "Q5)  What is the difference between Random Forest and decision trees?\n",
    "\n",
    "\n",
    "* Decision trees are a single tree-based model that predicts the outcome based on a set of rules. \n",
    "\n",
    "\n",
    "* Random Forest is an ensemble of multiple decision trees that work together to make a final prediction.\n",
    "\n",
    "\n",
    "* Random Forest is generally more accurate and less prone to overfitting than a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a986caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff7686be",
   "metadata": {},
   "source": [
    "Q6) How random Forest predict the outcome by multiple-decision trees?\n",
    "\n",
    "\n",
    "* In case of regresssion problem it will take the mean or average of all outputs and predict a final output.\n",
    "\n",
    "\n",
    "* In case of classification problem it will predict a outcome which is most frequent among all. Eg: out of 10 decision trees, 7 decision trees predict yes and 3 predict no. So, the final output will be yes because it is the most frequent result among all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc49013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10ad5eb5",
   "metadata": {},
   "source": [
    "Q7) How do you tune the parameters of a Random Forest model?\n",
    "\n",
    "* The main parameters that can be tuned in a Random Forest model are the number of trees(n_estimators), the maximum depth(max_depth) of the trees, and the number of features(max_features) used to build each decision tree on that number of features, the criterion used for splitting (criterion = 'gini' / 'entropy'), there are several more parameters available in random_forest.\n",
    "\n",
    "\n",
    "* The best values for these parameters can be found using techniques like grid search or random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c639b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6708b73",
   "metadata": {},
   "source": [
    "Q8) Q: Can Random Forest be used for feature selection?\n",
    "\n",
    "* Yes, Random Forest can be used for feature selection by ranking the importance of each feature and then, selecting the top features.\n",
    "\n",
    "\n",
    "* This can help to reduce the dimensionality of the data and improve the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861c346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7679051b",
   "metadata": {},
   "source": [
    "Q9) What are the advantages of using Random Forest over Decision Trees and other Machine Learning Algorithms?\n",
    "\n",
    "* Random Forest is a popular machine learning algorithm that can be used for both classification and regression tasks. Here are some of its advantages and disadvantages:\n",
    "\n",
    "Advantages:-\n",
    "\n",
    "1) High Accuracy:-  Random Forest is popular for its high accuracy due to its ability to reduce overfitting and the bias-variance trade-off.\n",
    "\n",
    "\n",
    "2) Random Forest is less sensitive to outliers, making it a robust algorithm. \n",
    "\n",
    "\n",
    "3) Random Forest can handle missing values and maintain accuracy in prediction.\n",
    "\n",
    "\n",
    "4) Random Forest provides a measure of feature importance, which can be useful for feature selection.\n",
    "\n",
    "\n",
    "5) The algorithm can handle large datasets with high dimensionality.\n",
    "\n",
    "\n",
    "Disadvantages:-\n",
    "\n",
    "\n",
    "1) Slow: Random Forest can be computationally expensive.\n",
    "\n",
    "\n",
    "2) Not interpretable:Random Forest is very challenging to understand that how the model is build and make predictions.\n",
    "\n",
    "\n",
    "3) Random Forest is generally robust to overfitting, it can still occur if the model is not properly tuned.\n",
    "\n",
    "\n",
    "4) Imbalanced data: Random Forest can struggle with imbalanced datasets, where one class dominates the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd8829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "955d5a76",
   "metadata": {},
   "source": [
    "Q10) What is Out-Of-Bag (OOB) concept in Random Forest?\n",
    "\n",
    "Out-of-bag (OOB) concept in Random Forest:-\n",
    "\n",
    "\n",
    "* Random Forest builds by combining multiple decision trees. Each decision tree trainned on random subset of data, which can create by bootstrapping.\n",
    "\n",
    "\n",
    "* Bootstrapping is a technique used in ensemble learning to create random subsets of data. Each subset of data is created by row and feature sampling with replacement.\n",
    "\n",
    "\n",
    "* Each subset contains approx 2/3 data and remaing 1/3 data is consider as OOB Data or validation Data.\n",
    "\n",
    "\n",
    "* We can use this OOB Data to evaulate the performance of model without seprating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1d5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13ab8733",
   "metadata": {},
   "source": [
    "Q11) What is OOB Score?\n",
    "\n",
    "\n",
    "* OOB Data is consider as validation data, which is used to evaluate the performance of model. \n",
    "\n",
    "\n",
    "* The accuracy score of OOB Data is known as OOB Score.\n",
    "\n",
    "\n",
    "* The advantage of using OOB Score is that we can evaulate the performance of model without seprating the dataset.\n",
    "\n",
    "\n",
    "* oob_score is the parameter in random forest used to evaulate the accuracy of OOB Data. By default it is False. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed188d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d757b6a",
   "metadata": {},
   "source": [
    "Q12) What is OOB Error?\n",
    "\n",
    "\n",
    "* The error of OOB Data is known as OOB error.\n",
    "\n",
    "\n",
    "* For example the oob score is 0.90 so, the oob error is 0.10.\n",
    "\n",
    "\n",
    "* A lower OOB error indicates better performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa79e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da4514e1",
   "metadata": {},
   "source": [
    "Q13) How Random Forest provides feature importance?\n",
    "\n",
    "\n",
    "* Random Forest checks the impurity of nodes by taking features. Afterthat, it will show the feature importance by measuring the impurity of nodes. Impurity check is done with the help of gini or entropy.\n",
    "\n",
    "\n",
    "* The feature which provides low impurity to nodes, having high importance value and vise-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219c8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90e91eff",
   "metadata": {},
   "source": [
    "Q14) Does random forest and decision tree requires scaling?\n",
    "\n",
    "\n",
    "* Decision trees and random forests do not require scaling because they are based on decision rules which doesn't need scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92551c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "933e1d8e",
   "metadata": {},
   "source": [
    "Q15)  What Are the Basic Assumption?\n",
    "* There are no such assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44848f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
